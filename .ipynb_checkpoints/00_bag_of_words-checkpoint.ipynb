{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/00_bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKEP5r0nOxjd"
   },
   "source": [
    "### 軽い前置き\n",
    "* この授業では、Pythonのコーディングの基礎は習得済みであることを前提します。\n",
    "* また、NumPyやscikit-learnの基本的な使い方は習得済みであることを前提します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AzUPIsj03QUI"
   },
   "source": [
    "# 00 bag-of-wordsモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qiozJha53etL"
   },
   "source": [
    "##00-00 bag-of-wordsモデルを取り巻く状況"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z57oKsOVGOWf"
   },
   "source": [
    "\n",
    "### 単語のmultisetとしての文書\n",
    "* **bag-of-wordsモデル**とは、文書をベクトルとしてモデル化する手法のひとつ。\n",
    " * 他にも文書をベクトル化する手法はある。\n",
    "* bag-of-wordsモデルにおいては、文書における単語トークンの**出現順序が無視される**。\n",
    " * 単語トークンとは、単語の一回一回の出現のこと。\n",
    "* つまり、文書を、バッグに入ったアイテムの集まりのようにモデリングする（下図参照）。\n",
    " * 言い換えれば、文書を単語の**multiset**として扱うのがbag-of-wordsモデルである。\n",
    "\n",
    "* 参考資料\n",
    " * https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-BOW.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBk-WNX4IRVb"
   },
   "source": [
    "![bag-of-words.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/bag-of-words.png)\n",
    "\n",
    "https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PilALQV12kVD"
   },
   "source": [
    "### 単語のベクトル表現の隆盛\n",
    "* しかし・・・最近では、言語データをモデル化するとき、必ずと言っていいほど、単語（あるいはsubword）のベクトル表現を用いる。\n",
    "* この単語ベクトル表現は、埋め込みembeddingとも呼ばれるし、分散表現とも呼ばれる。\n",
    "* 単語埋め込みが急速に普及したのは、深層学習の普及とほぼ同時期。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-5GrvauMKDq"
   },
   "source": [
    "### 過去のbag-of-wordsモデル\n",
    "* それまではbag-of-wordsが広く使われていた。\n",
    "* 論文では今でも、文書分類のbaselineとして、TF-IDFやBM25など、bag-of-wordsモデルが引き合いに出されることがある。\n",
    " * 例えば、文書分類の新手法を考え出してもbag-of-wordsに勝てなければ意味がない、といった使い方をされる。\n",
    " * 参考 https://twitter.com/moguranosenshi/status/1306406087445196800\n",
    "* そのため、授業の最初のトピックとして、bag-of-wordモデルについて簡単に説明しておく。\n",
    " * BM25については右リンクを参照。 https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Jn1BM6BJtei"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "興味がある方は、スタンフォード大の自然言語処理の授業が、この10年間でいかに大きく内容を変えているか、調べてみましょう。\n",
    "\n",
    "https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJZFJ9B0KNQO"
   },
   "source": [
    "## 00-01 binary vector\n",
    "* 最も単純には、文書は、語彙に含まれる各単語が出現するかしないかの2値ベクトルでモデル化できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uhf7MJe4NH3P"
   },
   "source": [
    "### scikit-learnのCountVectorizer\n",
    "* 各documentは、半角スペースでつながれた単語の列として準備しておく。\n",
    " * これをどうやって作るかは、後で説明する。\n",
    "* binaryをTrueにセットすると、0/1の2値ベクトルが得られる。\n",
    "* インスタンスを作り、fit_transformする、という使い方は、scikit-learnにおけるデータの前処理のときと同様。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAsV0OzFGKr8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSPMo5HnOeLK"
   },
   "source": [
    "### 文書の集合＝コーパスを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JvB4acfMxQq"
   },
   "outputs": [],
   "source": [
    "corpus = [\"This document is the first document.\",\n",
    "          \"This document is the second document.\",\n",
    "          \"And this is the third one.\",\n",
    "          \"Where is the fourth one?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9piYSlbPSan"
   },
   "source": [
    "### CountVectorizerをbinary=Trueで使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEcGpcc4M7bo"
   },
   "outputs": [],
   "source": [
    "binary_vectorizer = CountVectorizer(binary=True) # 2値ベクトルとして表現\n",
    "X = binary_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16VoXJLcNtmW"
   },
   "source": [
    "### 文書の2値ベクトル表現の確認\n",
    "* 疎なベクトルとして得られることに注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "8Nl4fjykNDo7",
    "outputId": "0172004a-40ee-46e7-e45e-cfc7954ade58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 6)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 5)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 3)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUICjJZ2N6px"
   },
   "source": [
    "### 疎な表現を通常のndarrayに戻す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "rJYPyDPDNsIL",
    "outputId": "d7be803e-f48e-4d85-e296-acee1fc7989b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XzbZQPHuOAIP"
   },
   "source": [
    "### 語彙を得る\n",
    "* 先頭の大文字は自動的に小文字に変換されていることが分かる。\n",
    "* ピリオドや疑問符は削除されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "oQAAI9LKN5vL",
    "outputId": "9dafe8d5-d893-4747-e810-53c73e4b3d8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'document': 1,\n",
       " 'is': 4,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'third': 8,\n",
       " 'one': 5,\n",
       " 'where': 10,\n",
       " 'fourth': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6P1dFVlcN_Pd",
    "outputId": "6f77b0b8-8f44-46cc-9753-c48c89cf7b4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'fourth', 'is', 'one', 'second', 'the', 'third', 'this', 'where']\n"
     ]
    }
   ],
   "source": [
    "print(binary_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4AeNQK2P1GW"
   },
   "source": [
    "### 新しい文書をベクトルに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "egmnVXnHPbkX"
   },
   "outputs": [],
   "source": [
    "new_doc = [\"This is the new document.\"]\n",
    "\n",
    "new_vectors = binary_vectorizer.transform(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHWGA6W4Psto"
   },
   "source": [
    "### 新出の単語は無視される点に注意\n",
    "* OoV (out-of-vocabulary) wordsの問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RqnzGOgmPj5B",
    "outputId": "a613fa22-4588-4bd5-f965-c48a2f11965e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTPClzcmP84l"
   },
   "source": [
    "## 00-02 word counts\n",
    "* 文書における各単語の出現回数を使って、文書のベクトル表現を得ることもできる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCBPOUUoQNoC"
   },
   "source": [
    "### CountVectorizerをデフォルト設定で使う\n",
    "* 単語の出現回数による文書のベクトル表現が得られる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZNhmGTuPmo4"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "3yzMyTrhQWmW",
    "outputId": "b1d5985c-af77-40e5-ddf3-aa05f1114edf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "eE_YpgstQfL_",
    "outputId": "3376a417-17f1-4b07-bd56-ff45a20d82fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vectors = count_vectorizer.transform(new_doc)\n",
    "new_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etJEdIKK5Ejo"
   },
   "source": [
    "## 00-03 TF-IDF\n",
    "* TF-IDFは、TFとIDFの積である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZZghSTZ5Gi_"
   },
   "source": [
    "### TF\n",
    "* 文書に含まれる単語トークンの数（つまり、単語の出現回数の総和）を、その文書の長さと呼ぶ。\n",
    "* TFとは、各々の単語が文書のなかで出現する回数を、その文書の長さで割ったものである。\n",
    " * 文書のなかで頻出する単語ほどTFは大きくなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7M88Lm1QwQF"
   },
   "source": [
    "### IDF\n",
    "* IDFとは、DFの逆数である。\n",
    "* DFとは、ある単語が含まれる文書の数を、総文書数で割ったものである。\n",
    " * 文書集合のなかで稀少な単語ほどIDFは大きくなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B13HQa1G5U_j"
   },
   "source": [
    "### TF-IDF\n",
    "* TF-IDFはTFとIDFの積であるが、積を求める前に、TFのルートを取ったり対数をとったり、あるいは、IDFのルートをとったり対数をとったりする。\n",
    " * 対数をとるときは、ゼロの対数をとることにならないような工夫をする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0RtY0PtSq3D"
   },
   "source": [
    "### TF-IDFの式の例\n",
    "\n",
    "\\begin{align}\n",
    "x_{d,w} = \\frac{n_{d,w}}{n_d} \\cdot ( 1 + \\ln\\frac{m}{m_w}) \\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    " * $n_{d,w}$ is the frequency of the word $w$ in the document $d$, \n",
    " * $n_d$ is defined as $n_d \\equiv \\sum_w n_{d,w}$,\n",
    " * $m_w$ is the number of documents containing the word $w$, and\n",
    " * $m$ is the total number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsLF9WdGHgFD"
   },
   "source": [
    "### TF-IDFの式のバリエーション\n",
    "\n",
    "![img462.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/img462.png)\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XfakOH8HqZ9"
   },
   "source": [
    "---\n",
    "\n",
    "* どの式の形がいいかは、downstream taskの性能をcross validationで評価して選ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N9_0aZB0TOnl"
   },
   "source": [
    "### scikit-learnのTfidfVectorizer\n",
    "* scikit-learnでのTF-IDFの計算式がどうなっているかは、下記ページを参照。\n",
    " * https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KisBcoHQjn8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "cA_F0D0tGMKZ",
    "outputId": "5f084c77-81a6-4d66-9d5d-2bb8dc0f9de8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# デフォルトの設定を確認してみる。\n",
    "# 次のパラメータは変更していいかもしれない。\n",
    "# 以下が重要！！\n",
    "# max_df, min_df, ngram_range, norm, smooth_idf, stop_words, sublinear_tf\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "aZYEcdYaTV28",
    "outputId": "95dd1d48-71e8-4fe5-b9a5-877a547ed7ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.74846041, 0.47466356, 0.        , 0.24769914,\n",
       "        0.        , 0.        , 0.24769914, 0.        , 0.3029716 ,\n",
       "        0.        ],\n",
       "       [0.        , 0.74846041, 0.        , 0.        , 0.24769914,\n",
       "        0.        , 0.47466356, 0.24769914, 0.        , 0.3029716 ,\n",
       "        0.        ],\n",
       "       [0.52898651, 0.        , 0.        , 0.        , 0.2760471 ,\n",
       "        0.41705904, 0.        , 0.2760471 , 0.52898651, 0.33764523,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.56199026, 0.29326983,\n",
       "        0.44307958, 0.        , 0.29326983, 0.        , 0.        ,\n",
       "        0.56199026]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6kSbn0uyUxOs",
    "outputId": "30c0290b-e36b-4246-9b15-72cd97ef0dff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文書ベクトルはL2ノルムが1となるように長さを変更されている。\n",
    "# （TfidfVectorizer()の引数normで変更可能。）\n",
    "\n",
    "import numpy as np\n",
    "np.linalg.norm(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "NPGd8r-pIAVl",
    "outputId": "bdec398f-7bbf-4a42-8cc9-7f3afd53ddcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'document',\n",
       " 'first',\n",
       " 'fourth',\n",
       " 'is',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this',\n",
       " 'where']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "fZ-M-qZjTYsL",
    "outputId": "a3d1a2f9-7d68-42d2-8fd4-d5f8248e0fb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.6284927 , 0.        , 0.        , 0.41599288,\n",
       "        0.        , 0.        , 0.41599288, 0.        , 0.50881901,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vectors = tfidf_vectorizer.transform(new_doc).toarray()\n",
    "new_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--XKc4eATjf_"
   },
   "source": [
    "### 各単語のIDF\n",
    "* IDFはそれぞれの単語について一意に決まる値。\n",
    "* 文書ごとに求まる値ではない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ugsq3YAtTfRx",
    "outputId": "46d9d234-9abd-4048-fd1d-a24de5895e9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.91629073, 1.51082562, 1.91629073, 1.91629073, 1.        ,\n",
       "       1.51082562, 1.91629073, 1.        , 1.91629073, 1.22314355,\n",
       "       1.91629073])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8niqN06T-eX"
   },
   "source": [
    "### bag-of-wordsベクトルの使い方\n",
    "* 例えば、文書どうしの類似度の計算に使える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "FP0KvdykThdg",
    "outputId": "f9d1c1b3-d4d4-4d8e-adee-9ab140d583d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8306417662781156\n",
      "0.8306417662781156\n",
      "0.401467570331823\n",
      "0.24399632162751606\n"
     ]
    }
   ],
   "source": [
    "# 長さ１にnormalizeされているので、内積がコサイン類似度に一致する。\n",
    "for i in range(4):\n",
    "  print(np.dot(X[i], new_vectors[0]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMsqrVa0b9uddw3QxmDUh7a",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "00_bag-of-words.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
