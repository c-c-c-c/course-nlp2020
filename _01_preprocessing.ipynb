{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/01_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIVXAmw_tMRv"
   },
   "source": [
    "# 01 テキストデータの扱い方：基本中の基本編\n",
    "\n",
    "* テキストデータは、長い長い文字列。\n",
    "* 長い長い文字列のままでは、普通は分析できない。\n",
    "* 今回は、自然言語処理における基本的な前処理について学ぶ。\n",
    "* また、今回は、英語データのみを扱う。\n",
    " * 日本語データは、次回、扱う。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXE1NNPpXbjI"
   },
   "source": [
    "今回のnotebook作成にあたって、下記のリポジトリを参考にしました。\n",
    "\n",
    " * https://github.com/dipanjanS/nlp_essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgeSwPsGJFWj"
   },
   "source": [
    "## 01-01 大文字小文字間の変換\n",
    "\n",
    "* Pythonの文字列型のメソッドを使えば、可能。\n",
    "\n",
    "* 問：元のテキストにあった大文字と小文字の区別を無くしてしまうことのメリットとデメリットは何か？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OQp382lJJFWp",
    "outputId": "bc84c694-433d-4130-b8e6-00adee0165e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over The Big Dog'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The quick brown fox jumped over The Big Dog'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FaAwb7HZJFWz",
    "outputId": "4878430b-f4f2-4a42-f7f2-982a6f8d38b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumped over the big dog'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ihX9LwVuJFW4",
    "outputId": "484a7ddc-8857-4f16-f3fc-cf7cd8657af8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE QUICK BROWN FOX JUMPED OVER THE BIG DOG'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "U24TBZ82JFW8",
    "outputId": "fe197368-e516-4c1a-e6c1-5f62739ba62d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Quick Brown Fox Jumped Over The Big Dog'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各トークンの一文字目を大文字にする。\n",
    "text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtCf5TIaJpEr"
   },
   "source": [
    "## 01-02 NLTKを使ってみる\n",
    "\n",
    "* NLTKは、Pythonの有名な自然言語処理ライブラリ。2001年スタートらしい。\n",
    "\n",
    "* https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "ceSG71XiJoka",
    "outputId": "b73be6cf-019b-4565-9b98-1fb809fd247f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3GzHq46JFW_"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "* 文に分ける、単語に分ける、など、長い文字列としての言語データをより小さな単位へと分割することを、一般にtokenizationと言う。\n",
    "* segmentationと言うこともある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "zIiPr5JBJFXA",
    "outputId": "d72863ba-3c91-4423-e761-973ecfba18e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pythonの文字列は、複数行にわたっていても、丸括弧でくくれば一つの長い文字列になる。\n",
    "# （ただし、最後の行を除いて、末尾に空白を入れておくのを忘れないように。）\n",
    "\n",
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "i2m8nEPmJFXD",
    "outputId": "9f5edeee-957d-47ea-e69c-330bf5e0ea82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
       " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
       " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
       " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文ごとに分割\n",
    "nltk.sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqCjdGno8FZ7"
   },
   "source": [
    "* 問：下に示すword tokenizationのメリットとデメリットは何か？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "KjVNIwLoJFXG",
    "outputId": "6ebf9093-afd8-450a-ef42-e95640e18f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
     ]
    }
   ],
   "source": [
    "# 単語ごとに分割\n",
    "print(nltk.word_tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D25190-Ft9Ls"
   },
   "source": [
    "## 01-03 spaCyを使ってみる\n",
    "\n",
    "* spaCyも、Pythonの有名な自然言語処理ライブラリ。2015年スタートらしい。\n",
    "\n",
    "* https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nS5twUUnuIPf"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjhORAuPJFXL"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DR6LA_YHJFXN"
   },
   "outputs": [],
   "source": [
    "text_spacy = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "QI_nN_l3zQwa",
    "outputId": "89f24723-b2c8-45d2-e23b-ff74516db807"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
       " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
       " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
       " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obj.text for obj in text_spacy.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RR2DcOE18LfI"
   },
   "source": [
    "* 問： 下のword tokenizationは、先ほどのword tokenizationとどう違うか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "DBuAHdR8JFXQ",
    "outputId": "ddc7480d-7186-4abc-fb98-ae4857abca67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating', 'the', 'previous', 'record', '-', 'holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
     ]
    }
   ],
   "source": [
    "print([obj.text for obj in text_spacy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhxnJkIsJFXS"
   },
   "source": [
    "## 01-04 HTML文書の前処理\n",
    "\n",
    "* __`<p>`__や__`<a>`__や__`<div>`__など、頻繁に使うHTMLタグは頭に入れておいてください。\n",
    "\n",
    "* なぜなら、ある程度HTMLタグが読めてはじめて、スクレイピングのコードを書くための、HTMLソースの下調べができるからです。\n",
    " * 自前でWeb上から分析対象のテキストデータを取得するときは、ダウンロードしようとするWebページのHTMLの構造を自分の目で確認する。\n",
    "\n",
    "* 問：誰かによって整備されたデータセットではなく、自前でHTML文書をスクレイピングすることのメリットとデメリットは何か？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ewyVIu3auUl"
   },
   "source": [
    "### HTML文書のダウンロード\n",
    "* いくつか方法はあるが、ここではrequestsモジュールを使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "E3qV1WOpJFXT",
    "outputId": "f8d0faa0-efbe-423f-b8c7-8523fb878259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id=\"id00011\" style=\"margin-top: 2em\">*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***</p>\r\n",
      "\r\n",
      "<p id=\"id00012\" style=\"margin-top: 4em\">This eBook was produced by David Widger\r\n",
      "with the help of Derek Andrew's text from January 1992\r\n",
      "and the work of Bryan Taylor in November 2002.</p>\r\n",
      "\r\n",
      "<h1 id=\"id00013\" style=\"margin-top: 5em\">Book 01        Genesis</h1>\r\n",
      "\r\n",
      "<p id=\"id00014\">01:001:001 In the beginning God created the heaven and the earth.</p>\r\n",
      "\r\n",
      "<p id=\"id00015\" style=\"margin-left: 0%; margin-right: 0%\">01:001:002 And the earth was without form, and void; and darkness was\r\n",
      "           upon the face of the deep. And the Spirit of God moved upon\r\n",
      "           the face of the waters.</p>\r\n",
      "\r\n",
      "<p id=\"id00016\">01:001:003 And God said, Let there be light: and there was light.</p>\r\n",
      "\r\n",
      "<p id=\"id00017\">01:001:004 And God saw the light, that it was good: and God divided the<br/>\r\n",
      "\r\n",
      "           light from the darkness.<br/>\r\n",
      "</p>\r\n",
      "\r\n",
      "<p id=\"id00018\">01:001:005 And God called the light Day, and the darkness he called<br/>\r\n",
      "\r\n",
      "           Night. And the evening and the morning were the first day.<br/>\r\n",
      "</p>\r\n",
      "\r\n",
      "<p id=\"id00019\">01:001:006 And God said, Let there be a firmament in the mi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "content = data.text\n",
    "print(content[2745:3948])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Vte11lpuXV_"
   },
   "source": [
    "### Beautiful Soupの利用\n",
    "\n",
    "* HTML文書の構造を解析するためによく使われるライブラリ。\n",
    "\n",
    "* 参考資料：「Beautiful Soup 4によるスクレイピングの基礎」\n",
    "\n",
    " * https://www.atmarkit.co.jp/ait/articles/1910/18/news015.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "E6UAz3mjJFXY",
    "outputId": "6bc556a8-ad18-49b2-a4b2-4a553e729fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\n",
      "This eBook was produced by David Widger\n",
      "with the help of Derek Andrew's text from January 1992\n",
      "and the work of Bryan Taylor in November 2002.\n",
      "Book 01        Genesis\n",
      "01:001:001 In the beginning God created the heaven and the earth.\n",
      "01:001:002 And the earth was without form, and void; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "01:001:003 And God said, Let there be light: and there was light.\n",
      "01:001:004 And God saw the light, that it was good: and God divided the\n",
      "           light from the darkness.\n",
      "01:001:005 And God called the light Day, and the darkness he called\n",
      "           Night. And the evening and the morning were the first day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    # 下の正規表現の意味を説明してみよう。\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[1163:1957])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "au1nuTllwX07"
   },
   "source": [
    "## 演習1-1\n",
    "* clean_contentを単語に分割し、各単語の出現頻度を求め、出現頻度の高い順に上位100の単語を、出現頻度とともに表示しよう。\n",
    "* clean_contentの内容をすべて小文字に変換した後で同じことをしてみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdGJ9gYO_j1o"
   },
   "outputs": [],
   "source": [
    "# 演習1-1の答案\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fJi5YyKJFXc"
   },
   "source": [
    "## 01-05 アクセント記号の除去\n",
    "\n",
    "* unicodedataというライブラリを使う。\n",
    "\n",
    " * https://docs.python.org/3/library/unicodedata.html\n",
    "\n",
    "* 'NFKD'は何を意味するか？ （Wikipedia「Unicode正規化」）\n",
    "\n",
    " * https://ja.wikipedia.org/wiki/Unicode%E6%AD%A3%E8%A6%8F%E5%8C%96\n",
    "\n",
    "* PythonにおけるUnicode HOWTO\n",
    "\n",
    " * https://docs.python.org/ja/3/howto/unicode.html\n",
    "\n",
    "* テキストデータの前処理においてアクセント記号を除去することのメリットとデメリットは何か？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps9wmhv9JFXd"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Mc7JR8CQJFXh",
    "outputId": "52237b22-9b83-4552-f51f-cac4cb3d815a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le bon sens est la chose du monde la mieux partagée; car chacun pense en être si bien pourvu, que ceux même qui sont les plus difficiles à contenter en toute autre chose n'ont point coutume d'en désirer plus qu'ils en ont.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = (\"Le bon sens est la chose du monde la mieux partagée; car chacun pense \"\n",
    "  \"en être si bien pourvu, que ceux même qui sont les plus difficiles à \"\n",
    "  \"contenter en toute autre chose n'ont point coutume d'en désirer plus \"\n",
    "  \"qu'ils en ont.\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "I6a-e-mVJFXm",
    "outputId": "75dc5262-d2ec-4aa3-a829-9e4750614aae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le bon sens est la chose du monde la mieux partagee; car chacun pense en etre si bien pourvu, que ceux meme qui sont les plus difficiles a contenter en toute autre chose n'ont point coutume d'en desirer plus qu'ils en ont.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_accented_chars(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gj8CyGmPJFXr"
   },
   "source": [
    "## 01-06 特殊文字、数字、記号の除去\n",
    "\n",
    "* reモジュールを使う。reはregular expression(正規表現)のこと。\n",
    "\n",
    "* 問：テキストデータの前処理において特殊文字、数字、記号などを除去することのメリットとデメリットは何か？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dkc4ESDJFXs"
   },
   "outputs": [],
   "source": [
    "# 問：下で使われている２つの正規表現はそれぞれどういう意味か？\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XUwKvQ-1JFXx",
    "outputId": "ede15681-b2c4-46b7-da2b-6140736c6c5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Sy9x4XFyJFYL",
    "outputId": "c148567e-a426-42f1-efa7-ffd4de444a35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun See you at  What do you think  '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(s, remove_digits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "s2vT0GK5JFYQ",
    "outputId": "ebe8bddc-5220-4c1b-80fd-53c1900425ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun See you at 730 What do you think 9318 '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_special_characters(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ho6h68QbJFYX"
   },
   "source": [
    "## 01-07 contraction\n",
    "\n",
    "* 英語には様々な省略表現がある。これを元に戻す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "mgGTT1URJFYY",
    "outputId": "6f55ab39-eede-4958-d19d-8a97b4ca7290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.0.25-py2.py3-none-any.whl (3.2 kB)\n",
      "Collecting textsearch\n",
      "  Downloading textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\n",
      "Requirement already satisfied: Unidecode in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\n",
      "Installing collected packages: textsearch, contractions\n",
      "Successfully installed contractions-0.0.25 textsearch-0.0.17\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: textsearch in /opt/conda/lib/python3.7/site-packages (0.0.17)\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.7/site-packages (from textsearch) (1.4.0)\n",
      "Requirement already satisfied: Unidecode in /opt/conda/lib/python3.7/site-packages (from textsearch) (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5xWsgO-jJFYc",
    "outputId": "79dfd64f-07ab-4fdd-abdc-5125a75c7391"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S2QTF2HFJFYi",
    "outputId": "04fb9ca9-ba0e-40b0-d223-0f73ad21ccb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "contractions_list = list(contractions.contractions_dict.items())\n",
    "print(len(contractions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Lm2e0y-Vymmg",
    "outputId": "a72b5f28-714a-465f-8ca6-e7ab3f88ad0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"ain't\", 'are not'), (\"aren't\", 'are not'), (\"can't\", 'can not'), (\"can't've\", 'can not have'), (\"'cause\", 'because'), (\"could've\", 'could have'), (\"couldn't\", 'could not'), (\"couldn't've\", 'could not have'), (\"didn't\", 'did not'), (\"doesn't\", 'does not'), (\"don't\", 'do not'), (\"hadn't\", 'had not'), (\"hadn't've\", 'had not have'), (\"hasn't\", 'has not'), (\"haven't\", 'have not'), (\"he'd\", 'he would'), (\"he'd've\", 'he would have'), (\"he'll\", 'he will'), (\"he'll've\", 'he will have'), (\"he's\", 'he is'), (\"how'd\", 'how did'), (\"how're\", 'how are'), (\"how'd'y\", 'how do you'), (\"how'll\", 'how will'), (\"how's\", 'how is'), (\"I'd\", 'I would'), (\"I'd've\", 'I would have'), (\"I'll\", 'I will'), (\"I'll've\", 'I will have'), (\"I'm\", 'I am'), (\"I've\", 'I have'), (\"isn't\", 'is not'), (\"it'd\", 'it would'), (\"it'd've\", 'it would have'), (\"it'll\", 'it will'), (\"it'll've\", 'it will have'), (\"it's\", 'it is'), (\"let's\", 'let us'), (\"ma'am\", 'madam'), (\"mayn't\", 'may not'), (\"might've\", 'might have'), (\"mightn't\", 'might not'), (\"mightn't've\", 'might not have'), (\"must've\", 'must have'), (\"mustn't\", 'must not'), (\"mustn't've\", 'must not have'), (\"needn't\", 'need not'), (\"needn't've\", 'need not have'), (\"o'clock\", 'of the clock'), (\"oughtn't\", 'ought not'), (\"oughtn't've\", 'ought not have'), (\"shan't\", 'shall not'), (\"sha'n't\", 'shall not'), (\"shan't've\", 'shall not have'), (\"she'd\", 'she would'), (\"she'd've\", 'she would have'), (\"she'll\", 'she will'), (\"she'll've\", 'she will have'), (\"she's\", 'she is'), (\"should've\", 'should have'), (\"shouldn't\", 'should not'), (\"shouldn't've\", 'should not have'), (\"so've\", 'so have'), (\"so's\", 'so is'), (\"that'd\", 'that would'), (\"that'd've\", 'that would have'), (\"that's\", 'that is'), (\"there'd\", 'there would'), (\"there'd've\", 'there would have'), (\"there's\", 'there is'), (\"they'd\", 'they would'), (\"they'd've\", 'they would have'), (\"they'll\", 'they will'), (\"they'll've\", 'they will have'), (\"they're\", 'they are'), (\"they've\", 'they have'), (\"to've\", 'to have'), (\"wasn't\", 'was not'), (\"we'd\", 'we would'), (\"we'd've\", 'we would have'), (\"we'll\", 'we will'), (\"we'll've\", 'we will have'), (\"we're\", 'we are'), (\"we've\", 'we have'), (\"weren't\", 'were not'), (\"what'll\", 'what will'), (\"what'll've\", 'what will have'), (\"what're\", 'what are'), (\"what's\", 'what is'), (\"what've\", 'what have'), (\"when's\", 'when is'), (\"when've\", 'when have'), (\"where'd\", 'where did'), (\"where's\", 'where is'), (\"where've\", 'where have'), (\"who'll\", 'who will'), (\"who'll've\", 'who will have'), (\"who's\", 'who is'), (\"who've\", 'who have'), (\"why's\", 'why is'), (\"why've\", 'why have'), (\"will've\", 'will have'), (\"won't\", 'will not'), (\"won't've\", 'will not have'), (\"would've\", 'would have'), (\"wouldn't\", 'would not'), (\"wouldn't've\", 'would not have'), (\"y'all\", 'you all'), (\"y'all'd\", 'you all would'), (\"y'all'd've\", 'you all would have'), (\"y'all're\", 'you all are'), (\"y'all've\", 'you all have'), (\"you'd\", 'you would'), (\"you'd've\", 'you would have'), (\"you'll\", 'you will'), (\"you'll've\", 'you shall have'), (\"you're\", 'you are'), (\"you've\", 'you have'), ('jan.', 'january'), ('feb.', 'february'), ('mar.', 'march'), ('apr.', 'april'), ('jun.', 'june'), ('jul.', 'july'), ('aug.', 'august'), ('sep.', 'september'), ('oct.', 'october'), ('nov.', 'november'), ('dec.', 'december'), ('ain’t', 'are not'), ('aren’t', 'are not'), ('can’t', 'can not'), ('can’t’ve', 'can not have'), ('’cause', 'because'), ('could’ve', 'could have'), ('couldn’t', 'could not'), ('couldn’t’ve', 'could not have'), ('didn’t', 'did not'), ('doesn’t', 'does not'), ('don’t', 'do not'), ('hadn’t', 'had not'), ('hadn’t’ve', 'had not have'), ('hasn’t', 'has not'), ('haven’t', 'have not'), ('he’d', 'he would'), ('he’d’ve', 'he would have'), ('he’ll', 'he will'), ('he’ll’ve', 'he will have'), ('he’s', 'he is'), ('how’d', 'how did'), ('how’re', 'how are'), ('how’d’y', 'how do you'), ('how’ll', 'how will'), ('how’s', 'how is'), ('I’d', 'I would'), ('I’d’ve', 'I would have'), ('I’ll', 'I will'), ('I’ll’ve', 'I will have'), ('I’m', 'I am'), ('I’ve', 'I have'), ('isn’t', 'is not'), ('it’d', 'it would'), ('it’d’ve', 'it would have'), ('it’ll', 'it will'), ('it’ll’ve', 'it will have'), ('it’s', 'it is'), ('let’s', 'let us'), ('ma’am', 'madam'), ('mayn’t', 'may not'), ('might’ve', 'might have'), ('mightn’t', 'might not'), ('mightn’t’ve', 'might not have'), ('must’ve', 'must have'), ('mustn’t', 'must not'), ('mustn’t’ve', 'must not have'), ('needn’t', 'need not'), ('needn’t’ve', 'need not have'), ('o’clock', 'of the clock'), ('oughtn’t', 'ought not'), ('oughtn’t’ve', 'ought not have'), ('shan’t', 'shall not'), ('sha’n’t', 'shall not'), ('shan’t’ve', 'shall not have'), ('she’d', 'she would'), ('she’d’ve', 'she would have'), ('she’ll', 'she will'), ('she’ll’ve', 'she will have'), ('she’s', 'she is'), ('should’ve', 'should have'), ('shouldn’t', 'should not'), ('shouldn’t’ve', 'should not have'), ('so’ve', 'so have'), ('so’s', 'so is'), ('that’d', 'that would'), ('that’d’ve', 'that would have'), ('that’s', 'that is'), ('there’d', 'there would'), ('there’d’ve', 'there would have'), ('there’s', 'there is'), ('they’d', 'they would'), ('they’d’ve', 'they would have'), ('they’ll', 'they will'), ('they’ll’ve', 'they will have'), ('they’re', 'they are'), ('they’ve', 'they have'), ('to’ve', 'to have'), ('wasn’t', 'was not'), ('we’d', 'we would'), ('we’d’ve', 'we would have'), ('we’ll', 'we will'), ('we’ll’ve', 'we will have'), ('we’re', 'we are'), ('we’ve', 'we have'), ('weren’t', 'were not'), ('what’ll', 'what will'), ('what’ll’ve', 'what will have'), ('what’re', 'what are'), ('what’s', 'what is'), ('what’ve', 'what have'), ('when’s', 'when is'), ('when’ve', 'when have'), ('where’d', 'where did'), ('where’s', 'where is'), ('where’ve', 'where have'), ('who’ll', 'who will'), ('who’ll’ve', 'who will have'), ('who’s', 'who is'), ('who’ve', 'who have'), ('why’s', 'why is'), ('why’ve', 'why have'), ('will’ve', 'will have'), ('won’t', 'will not'), ('won’t’ve', 'will not have'), ('would’ve', 'would have'), ('wouldn’t', 'would not'), ('wouldn’t’ve', 'would not have'), ('y’all', 'you all'), ('y’all’d', 'you all would'), ('y’all’d’ve', 'you all would have'), ('y’all’re', 'you all are'), ('y’all’ve', 'you all have'), ('you’d', 'you would'), ('you’d’ve', 'you would have'), ('you’ll', 'you will'), ('you’ll’ve', 'you shall have'), ('you’re', 'you are'), ('you’ve', 'you have')]\n"
     ]
    }
   ],
   "source": [
    "print(contractions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KoIGJXqCJFYo",
    "outputId": "99cede2d-5e28-46bd-ed87-8826bdef1e28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you all can not expand contractions I would think! You would not be able to. how did you do it?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bC54E8pjzGMu",
    "outputId": "b10fd826-0be5-4636-d75e-e30ff6f44a74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is pool-season from this week, is not it? Oh yes. I have got to go and buy a swimming suit, then.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"It's pool-season from this week, isn't it? Oh yes. I've gotta go and buy a swimming suit, then.\"\n",
    "contractions.fix(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeUHPmhDJFZC"
   },
   "source": [
    "## 01-08 NLTKでstemming\n",
    "\n",
    "* 語尾が変化する単語の、その変化を無くして、語幹を得る。\n",
    "* 得られる語幹は、英単語として通用しない文字列になることが多い。\n",
    "* Stemming and Lemmatization in Python\n",
    " * https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "\n",
    "* 問：テキストデータの前処理としてstemmingをすることのメリットとデメリットは何か？\n",
    "* 問：様々な種類のstemmerがあるのはなぜか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8ndJ4XOKJFZD",
    "outputId": "c1aea33a-192f-41fd-ceba-37af3335f701"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Porter Stemmerを使ってみる （stemmerと言えばこれ、というぐらい良く知られている。）\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CmWLISH-JFZG",
    "outputId": "fe8a33a4-627a-4713-b921-2268ac9e1bcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lie'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Q7KRj1jtJFZJ",
    "outputId": "f1dada9b-0656-4828-c97b-a44bdf13d68c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strang'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQNUmpfLJFZu"
   },
   "source": [
    "## 01-09 NLTKでlemmatization（不完全版）\n",
    "\n",
    "* 語形が変わる単語を原型に戻す。\n",
    "* 原型は、英語の単語として通用する。\n",
    "\n",
    "* 問：テキストデータの前処理としてlemmatizationをすることのメリットとデメリットは何か？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16ygP7t1JFZv"
   },
   "outputs": [],
   "source": [
    "# WordNetを辞書として使うlemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "AieUIjYaJFZ3",
    "outputId": "b9f36af6-5af2-4ebd-b36d-421798f8327c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmatize in module nltk.stem.wordnet:\n",
      "\n",
      "lemmatize(word, pos='n') method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wnl.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "_ZPcwz44JFZ7",
    "outputId": "a5c72f36-491a-44c7-8251-f33b22d551fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "box\n"
     ]
    }
   ],
   "source": [
    "# 名詞\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('boxes', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "KJN-uQ28JFZ_",
    "outputId": "ad38f894-5d98-47f4-a3d9-398f6dc9fb34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "# 動詞\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "L0u5uZeoJFaF",
    "outputId": "ec8b25ae-ef5f-4541-c299-fc654e0dd7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "# 形容詞\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "NhKXkdckJFaN",
    "outputId": "22a75d95-37e3-4ef5-b5d0-ac177140f22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# 指定した品詞が間違っていると、うまくいかない。\n",
    "# （品詞を取得する方法は、すぐ後で解説する。）\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))\n",
    "print(wnl.lemmatize('fancier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQ1S2ngz7B84"
   },
   "source": [
    "## 01-10 NLTKによるtokenizationとlemmatizationの組み合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0l372SiEJFaU",
    "outputId": "2eb567ed-4466-429e-f138-02154293a66e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n"
     ]
    }
   ],
   "source": [
    "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
    "\n",
    "tokens = nltk.word_tokenize(s)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "s1FHAghFJFaX",
    "outputId": "d860770c-1aa2-4901-ed0c-f0bd2127e9af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox are quick and they are jumping over the sleeping lazy dog !'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ここでのlemmatizationは、品詞情報を使っていないので、不完全。下でこれを改良する。\n",
    "\n",
    "lemmatized_text = ' '.join(wnl.lemmatize(token) for token in tokens)\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0-fgmbi7E5_"
   },
   "source": [
    "## 01-11 POS Tagging\n",
    "\n",
    "* 問：品詞の情報が必要になるのはどういうときか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "UDffFU3gJFaZ",
    "outputId": "a7243111-6789-4ce7-b62a-5a48c9d05205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9STnRHVt7HRG"
   },
   "source": [
    "### NLTKが与えるPOSタグをWordNetのPOSタグに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2S9kS_xPJFaf"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "TbijTK6YJFaj",
    "outputId": "04969ecf-9630-4601-dbde-3ebac768c320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'n'), ('brown', 'a'), ('foxes', 'n'), ('are', 'v'), ('quick', 'a'), ('and', 'n'), ('they', 'n'), ('are', 'v'), ('jumping', 'v'), ('over', 'n'), ('the', 'n'), ('sleeping', 'v'), ('lazy', 'a'), ('dogs', 'n'), ('!', 'n')]\n"
     ]
    }
   ],
   "source": [
    "wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "print(wordnet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKia_-ov7KLH"
   },
   "source": [
    "## 01-12 NLTKでlemmatization（完全版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tNOpTLDTJFal",
    "outputId": "170c7da3-61f3-44e3-f83b-ef7695f10ff9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zisZIs1JFan"
   },
   "source": [
    "## 演習1-2\n",
    "\n",
    "* 上の3つのセルでおこなった処理をまとめて一つの関数として定義しよう。\n",
    " - 関数 __`wordnet_lemmatize_text()`__ を定義する。\n",
    " - 入力は変数 __`text`__ とし、これは文字列とする。\n",
    " - この関数のなかで、さきほど定義した関数__`pos_tag_wordnet()`__を使う。\n",
    " - そして、lemmatizeされたテキストを文字列型の出力として返すようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPovRPZb5PVG"
   },
   "outputs": [],
   "source": [
    "# 演習1-2の答案\n",
    "# def wordnet_lemmatize_text(text):\n",
    "#   ..........\n",
    "\n",
    "\n",
    "\n",
    "#s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
    "#wordnet_lemmatize_text(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgQJp2SH7OC_"
   },
   "source": [
    "## 01-13 spaCyでlemmatization\n",
    "\n",
    "* 上のように、別途品詞を調べる必要はない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3N2ExlFqJFaw"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en', parse=False, tag=False, entity=False)\n",
    "\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ga-E47JKJFaz",
    "outputId": "5f9c3d97-d391-48b1-97c8-05c6d41db872"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Bb-PrIeqJFa5",
    "outputId": "dcbcad3f-8c78-4955-82a0-81577584d0e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_lemmatize_text(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQsKAXlvJFa7"
   },
   "source": [
    "## 01-14 ストップワードの除去\n",
    "\n",
    "* ストップワードとは、言語データを分析するにあたって、非常に頻繁に使われるため内容の分析にあまり役に立たない単語のことを言う。\n",
    "\n",
    "* これこそが英語のストップワードだ！と言えるような決定的なストップワードのリストがあるわけではない。\n",
    "\n",
    " * 主要なNLPライブラリでは、あらかじめ用意されたストップワードのリストを使うことができる。\n",
    "\n",
    " * しかし、分析したいテキストデータに合わせて、ストップワードのリストをカスタマイズすることも、よくある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "9TaYuRW2AxvE",
    "outputId": "635d38b3-7945-47a5-a359-ab88e8f2742c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'either', 'four', 'yours', 'around', 'hereafter', 'along', 'who', 'has', 'least', 'beside', '‘ll', 'go', 'besides', 'whither', 'whereas', '‘m', 'ours', 'seeming', 'which', 'other', 'were', 'to', 'often', 'rather', 'of', 'quite', 'sixty', 'so', 'n‘t', 'show', 'would', 'anything', 'his', 'nothing', 'since', 'whose', 'as', 'and', 'perhaps', 'somehow', 'otherwise', 'everything', 'keep', 'regarding', 'get', 'was', 'yet', 'call', 'next', 'hundred', 'per', 'whenever', 'he', 'only', 'whoever', 'did', 'sometime', 'it', 'less', 'why', 'seem', 'ca', 'most', 'somewhere', '’re', 'without', 'former', 'still', 'hereupon', 'anyway', 'used', 'again', '’ll', 'across', 'done', 'others', 'much', 'ourselves', 'become', \"'ll\", 'itself', 'that', 'because', 'enough', 'do', 'even', 'more', \"'d\", '’ve', 'have', 'several', 'nowhere', 'none', 'sometimes', 'had', 'being', 'thereby', 'never', 'just', 'whereupon', 'him', 'two', \"'ve\", 'each', 'back', 'hers', 'if', 'also', 'part', 'should', 'anyone', 'thence', 'below', 'into', 'doing', 'noone', 'on', 'onto', 'its', 'another', 'for', 'could', 'will', 'toward', 'neither', 'some', 'well', 'move', 'among', 'with', 'off', 'how', '‘s', 'here', 'third', 'her', 'you', 'formerly', 'few', 'too', 'mine', 'both', '‘re', 'not', 'up', 'hereby', 'something', 'whereby', 'i', 'she', 'really', 'thereafter', 'twenty', 'does', 'are', 'becoming', 'there', 'one', 'those', 'whom', 'must', 'them', 'himself', 'thru', 'where', 'already', 'such', 'until', 'they', 'first', 'moreover', 'elsewhere', 'fifteen', 'although', 'thereupon', 'might', 'take', 'now', 'the', '‘ve', 'whereafter', 'seemed', 'during', 'eleven', 'give', 'six', 'against', 'seems', 'their', 'serious', 'your', 'over', 'about', 'amount', 'throughout', 'behind', 'beforehand', 'in', 'hence', 'herself', 'twelve', 'when', 'therein', 'once', 'themselves', 'due', \"n't\", 'no', 'became', 'be', 'by', 'whatever', 'three', \"'m\", '’m', 'beyond', 'using', 'thus', 'indeed', 'ever', 'us', 'whole', 'this', 'any', 'someone', 'becomes', 'my', 'various', 'own', 'eight', 'while', 're', 'can', '’d', 'wherever', 'top', '‘d', '’s', 'nobody', 'all', 'may', 'above', 'am', 'whether', 'almost', 'latterly', 'therefore', 'made', 'yourself', 'many', 'is', \"'s\", 'empty', 'everywhere', 'however', 'afterwards', 'down', 'towards', 'put', 'or', 'nine', 'further', 'our', 'anyhow', 'alone', 'through', 'before', 'everyone', 'bottom', 'meanwhile', 'at', 'nevertheless', 'name', 'though', 'else', 'upon', 'always', 'five', 'namely', 'than', 'please', 'after', 'side', 'see', 'between', 'whence', 'n’t', 'been', 'via', 'cannot', 'anywhere', 'out', 'fifty', 'myself', 'front', 'nor', 'from', 'forty', 'same', 'amongst', 'unless', 'a', 'say', \"'re\", 'an', 'make', 'under', 'me', 'within', 'latter', 'except', 'what', 'these', 'but', 'ten', 'very', 'together', 'then', 'every', 'full', 'last', 'herein', 'we', 'wherein', 'yourselves', 'mostly'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS)\n",
    "print(len(STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkJLKKxrJFa7"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def remove_stopwords(text, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    tokens = [obj.text for obj in nlp(text)]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ycusSsPBJFbA",
    "outputId": "f74ffdbd-10b7-4022-b57a-62dd688c93ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oWKjTPnzJFbD",
    "outputId": "799934f3-6832-4339-c0f8-0ad9c0c69a6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown foxes quick jumping sleeping lazy dogs !'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVfSpyrrCJ63"
   },
   "source": [
    "## 01-99 segmentationについて\n",
    "* 図表は下記のブログ記事より。\n",
    " * https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html\n",
    "\n",
    "![Segmentation.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/Segmentation.png)\n",
    "![inherent_task_complexity.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/inherent_task_complexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj8RJ6EoCwgp"
   },
   "source": [
    "# 課題1\n",
    "\n",
    "* Wikipediaの適当な英語のエントリをダウンロードする。\n",
    "\n",
    " * 選ぶのが面倒という方はAIのエントリでもどうぞ。\n",
    "   * https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "\n",
    "* BeautifulSoupで本文のテキストだけを取得する。\n",
    "\n",
    " * HTMLのソースを見て、どこが本文かを確認する。\n",
    " * あるいは、ネット検索をして、Wikipediaのエントリから本文だけを取得する方法を調べる。\n",
    "\n",
    "* 以下の前処理をする。\n",
    "\n",
    " * 大文字は小文字にする。ただし固有名詞を除く。\n",
    "\n",
    " * ストップワードを除去する。\n",
    "\n",
    " * lemmatizationする。\n",
    "\n",
    "* 各単語の出現回数を求め、表示する。\n",
    "\n",
    "* lemmatizationした後の単語を、元の出現順序どおりに、半角スペースで区切ってつなぎ、長い一つの文字列にする。\n",
    " * joinメソッドを使えばよい。つまり、__`' '.join(`__ lemmatizeされた単語のリスト __`)`__ という感じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zDMb9o6cgMg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
