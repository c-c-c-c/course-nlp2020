{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 fastTextのword embeddingsを使ってみる\n",
    "* このnotebookは、Google Colabではなく、手元の環境で動かすことを想定しています。\n",
    " * Google Colabで動かすとかなり時間がかかると思います。\n",
    "* 必要ならば、このnotebookを実行する前に、condaの環境を作っておきましょう。\n",
    "\n",
    "`$ conda create -n D_wordvec`\n",
    "\n",
    "`$ source activate D_wordvec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-01 fastTextをインストールする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Word vectors for 157 languages\"から英語データをダウンロード\n",
    "* fastTextのドキュメント https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "* 論文 https://arxiv.org/abs/1802.06893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc.en.300.bin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-02 IMDbデータセットをダウンロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本家のサイトからダウンロード\n",
    "* 方法は他にもあるが、ここでは本家サイトから直にダウンロードする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-17 03:18:32--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
      "\n",
      "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  10.7MB/s    in 12s     \n",
      "\n",
      "2020-10-17 03:18:45 (6.52 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ml-datasetsをインストール\n",
    "* https://pypi.org/project/ml-datasets/\n",
    "* 機械学習のデータセットのローダ。IMDbも簡単に扱える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-datasets in /opt/conda/lib/python3.7/site-packages (0.1.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from ml-datasets) (4.43.0)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from ml-datasets) (1.18.1)\n",
      "Requirement already satisfied: catalogue<3.0.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ml-datasets) (1.0.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from ml-datasets) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<3.0.0,>=0.2.0->ml-datasets) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<3.0.0,>=0.2.0->ml-datasets) (3.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ml-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastTextの単語ベクトルを読み込む\n",
    "* さきほどダウンロードし、解凍しておいたものを読み込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# loading cc.en.300.bin ...\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "model_path = 'cc.en.300.bin'\n",
    "print(f'# loading {model_path} ...', flush=True) \n",
    "ft = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDbデータセットを読み込む\n",
    "* 本家サイトからダウンロードし、解凍しておいたものを、ml_datasetsを使って読み込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_datasets import imdb\n",
    "\n",
    "train_valid_data, test_data = imdb(loc='./aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Once again, Pia Zadora, the woman who owes her entire career to her husband, proves she can't act. This disaster of a film butchers the Harold Robbins novel. Ray Liotta must have been hogtied and carried to the set to appear in this one.\\n\\n\\n\\nAvoid this at all costs. I doubt even doing the MST3K thing would save it.\",\n",
       "  0),\n",
       " ('Just finished this movie... saw it on the video shelf and being a Nick Stahl fan I just had to rent it. In all honesty, it probably should have stayed on the shelf. The concept was an interesting one and there were several fairly smart twists and turns but somehow I guessed almost all of them before they came along. And the movie just went a little too far in the end in my opinion... if you have to suffer through a viewing of it you\\'ll see what I mean!\\n\\n\\n\\nOn a positive note, Nick Stahl\\'s acting was great (especially considering what he had to work with). Eddie Kaye Thomas was also good but he always plays the same type of character... too much Paul Finch from \"American Pie\" coming through for my liking.\\n\\n\\n\\nAnd finally, the worst part of this movie has to be January Jones\\' emotionless performance... I guess a pretty face really is all that matters in Hollywood.',\n",
       "  0),\n",
       " (\"I don't see what everyone liked about this movie. The set-up was too long and talky, and when it was done, the main character remained as flat and opaque as he had been in the first scene. After the film finally got Cusack into the eponymous hotel room, I had to wonder, well, what's going to happen here for the next hour or so to keep me engaged. The answer: not much, just John Cusack having a long, drawn-out, mental breakdown.\\n\\n\\n\\nMaybe if the Cusack character had more depth . . maybe if his freak-out were a more thorough reworking of his everyday life . . . maybe if the film had either better developed its half-baked themes about loss and faith or had not tacked them on in the first place . . . maybe if the film had made a choice to be either psychological horror or thrill-ride horror and had fully embraced one of these styles . . . I dunno. All I do know is that I saw this movie with two other horror buffs and none of us much liked it.\\n\\n\\n\\nExcept for the disquieting episode on the hotel ledge, the alarming crazy lady with the hammer, and the so-stupid-it-was-fun crypt keeper in the air duct, all three of which account for no more than five minutes of screen time, this film was a bore.\\n\\n\\n\\nBy the way, this story seems to steal ideas from The Shining and use them here to much less powerful effect. Is Stephen King now reduced to stealing ideas from himself?\",\n",
       "  0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキスト部分と0/1ラベル部分に分ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_texts, train_valid_labels = zip(*train_valid_data)\n",
    "test_texts, test_labels = zip(*test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストセット以外をランダムにシャッフル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(train_valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手動で訓練データと検証データへ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(train_valid_data) * 0.8)\n",
    "train_texts, train_labels = train_valid_texts[:split], train_valid_labels[:split]\n",
    "valid_texts, valid_labels = train_valid_texts[split:], train_valid_labels[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 20000 training, 5000 validation, and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "print(f'# {len(train_texts)} training, {len(valid_texts)} validation, and {len(test_texts)} test docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    'train': (train_texts, train_labels),\n",
    "    'valid': (valid_texts, valid_labels),\n",
    "    'test': (test_texts, test_labels)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全文書のembeddingを得てファイルに保存\n",
    "* fastTextのget_sentence_vectorを使って文書のベクトル表現を得る。\n",
    "* 全文書のベクトル表現をndarrayに変換、`.npy`形式で保存\n",
    "* 全文書のラベルもndarrayに変換、`.npy`形式で保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train set: ---------*---------*\n",
      "# valid set: -----\n",
      "# test set: ---------*---------*-----\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for tag in splits:\n",
    "    print(f'# {tag} set: ', end='', flush=True)\n",
    "    cnt = 0\n",
    "    X = list()\n",
    "    for text in splits[tag][0]:\n",
    "        vec = ft.get_sentence_vector(' '.join(text.split('\\n')))\n",
    "        X.append(vec)\n",
    "        cnt += 1\n",
    "        if cnt % 10000 == 0: print('*', end='', flush=True)\n",
    "        elif cnt % 1000 == 0: print('-', end='', flush=True)\n",
    "    X = np.array(X)\n",
    "    with open(f'{tag}.npy', 'wb') as f:\n",
    "        np.save(f, X, allow_pickle=False)\n",
    "    with open(f'{tag}_labels.npy', 'wb') as f:\n",
    "        np.save(f, np.array(splits[tag][1]), allow_pickle=False)\n",
    "    print(flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-rw- 1 root root 30000128 Oct 10 11:23 test.npy\r\n",
      "-rw-rw-rw- 1 root root   200128 Oct 10 11:23 test_labels.npy\r\n",
      "-rw-rw-rw- 1 root root 24000128 Oct 10 11:23 train.npy\r\n",
      "-rw-rw-rw- 1 root root   160128 Oct 10 11:23 train_labels.npy\r\n",
      "-rw-rw-rw- 1 root root  6000128 Oct 10 11:23 valid.npy\r\n",
      "-rw-rw-rw- 1 root root    40128 Oct 10 11:23 valid_labels.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al *.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
