{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/07_document_classification_20201205.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUBLAS_WORKSPACE_CONFIG=\":4096:8\"\n",
    "# CUBLAS_WORKSPACE_CONFIG=\":16:8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_0ZZ8bo1mgH"
   },
   "source": [
    "# 07 単語埋め込みを使った文書分類\n",
    "* 今回も、IMDbデータセットの感情分析を文書分類問題として解く。\n",
    "* ただし今回は、fastTextのような学習済みの単語埋め込みは使わない。\n",
    "* 単語埋め込み自体の学習も、ネットワークの学習と同時におこなう。\n",
    "* IMDbデータの準備も、`torch.torchtext`を使っておこなう。\n",
    " * つまりすべてをPyTorchのなかでおこなう。\n",
    "* 参考資料\n",
    " * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
    " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyjU004LNbMt"
   },
   "source": [
    "## データをどう扱うか\n",
    "* ネットワークへの入力は、単語埋め込みを、単語の出現順どおりに並べた列にする。\n",
    " * ミニバッチは[ミニバッチのなかでの最大文書長, ミニバッチのサイズ, 単語埋め込み次元数]という形の3階のテンソルになる。\n",
    "* そして、前向き計算のなかではじめて、単語埋め込みの平均をとることにする。\n",
    " * `.mean(0)`と、軸0で平均をとることになる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_puYg6Zi8x3"
   },
   "source": [
    "## 07-00 Google Colabのランタイムのタイプを変更する\n",
    "* Google ColabのランタイムのタイプをGPUに変更しておこう。\n",
    " * 上のメニューの「ランタイム」→「ランタイムのタイプを変更」→「ハードウェア　アクセラレータ」から「GPU」を選択"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLEeO0fw23Xp"
   },
   "source": [
    "## 07-01 torchtextを使ってIMDbデータを読み込む\n",
    "* ここでIMDbデータセットの読み込みにつかう`torchtext.datasets`については、下記を参照。\n",
    " * https://torchtext.readthedocs.io/en/latest/datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go7epLZe3JmF"
   },
   "source": [
    "### 実験の再現性確保のための設定など\n",
    "* https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6nSqNzof1lTJ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y1_GyXg22f6"
   },
   "source": [
    "### torchtextのフィールド\n",
    "* TEXTフィールドと、LABELフィールドという２種類のFieldオブジェクトのインスタンスを作る。\n",
    " * Fieldクラスの詳細については[ここ](https://github.com/pytorch/text/blob/master/torchtext/data/field.py)を参照。\n",
    "* TEXTフィールドは、テキストの前処理の仕方を決めておくのに使う。\n",
    " * tokenizerは、デフォルトでは単にstring型のsplitメソッドを適用するだけになる。これは高速だが、tokenizationとしては雑。\n",
    "* LABELフィールドは、ラベルの前処理に使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qjq8oooE2uQY"
   },
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=\"spacy\")\n",
    "LABEL = LabelField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtEq23GS3Vxl"
   },
   "source": [
    "### IMDbデータセットをダウンロードした後、前処理しつつ読み込む\n",
    "* ダウンロードはすぐ終わるが、解凍に少し時間がかかる。\n",
    "* また、TEXTフィールドでspaCyのtokenizationを使うように設定したので、少し時間がかかる。\n",
    " * string型のsplitメソッドでtokenizeすると、時間はあまりかからない。（そのかわり、やや雑なtokenizationになる。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzgVXf3G3YPI",
    "outputId": "fcdf4bcd-4698-4284-902a-af4d2a2f8592"
   },
   "outputs": [],
   "source": [
    "train_valid_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0sltPjT3j36"
   },
   "source": [
    "### 最初の文書を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrXwYMVH3orf",
    "outputId": "76e2254d-bee8-4634-8da4-0eb9ebf8fa01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'disagree', 'strongly', 'with', 'anyone', 'who', 'might', 'dismiss', 'this', 'film', 'as', '\"', 'just', '\"', 'entertainment', '.', 'Set', 'right', 'after', 'the', 'carefree', ',', 'roaring', '20s', ',', 'during', 'the', 'early', 'days', 'of', 'the', 'Great', 'Depression', ',', 'Dance', ',', 'Fools', ',', 'Dance', 'is', 'at', 'its', 'heart', 'an', 'earnest', 'cautionary', 'tale', ',', 'with', 'a', 'clear', 'message', 'about', 'how', 'best', 'to', 'endure', 'these', 'hard', 'times', '.', 'Yet', 'this', 'fast', '-', 'paced', 'and', 'tightly', '-', 'plotted', 'film', 'is', 'far', 'from', 'being', 'a', 'dreary', 'morality', 'tale.<br', '/><br', '/>In', 'the', '30s', ',', 'Hollywood', 'had', 'a', 'knack', 'for', 'churning', 'out', 'one', 'entertaining', '*', 'and', '*', 'enlightening', 'audience', '-', 'pleaser', 'after', 'another', ',', 'all', 'without', 'wasting', 'a', 'frame', 'of', 'film', '.', 'Dance', ',', 'Fools', ',', 'Dance', '--', 'one', 'of', '*', 'four', '*', 'films', 'that', 'Harry', 'Beaumont', 'directed', 'in', '1931', '--', 'is', 'barely', '80', 'minutes', 'long', ',', 'yet', 'its', 'characters', 'are', 'well', 'developed', ',', 'its', 'story', 'never', 'seems', 'rushed', ',', 'and', 'despite', 'its', 'many', 'twists', 'in', 'plot', ',', 'the', 'audience', 'is', 'never', 'left', 'behind.<br', '/><br', '/>With', 'the', 'lone', 'exception', 'of', 'Lester', 'Vail', 'as', 'flaccid', 'love', 'interest', 'Bob', 'Townsend', ',', 'the', 'supporting', 'cast', 'is', 'uniformly', 'strong', '.', 'Worthy', 'of', 'note', 'are', 'William', 'Bakewell', 'as', 'Crawford', \"'s\", 'brother', ',', 'Cliff', 'Edwards', '(', 'best', 'known', 'as', 'the', 'voice', 'of', 'Jiminy', 'Cricket', ')', 'as', 'reporter', 'Bert', 'Scranton', ',', 'and', 'Clark', 'Gable', 'in', 'an', 'early', 'supporting', 'role', 'as', 'gangster', 'Jake', 'Luva.<br', '/><br', '/>But', 'this', 'is', 'Joan', 'Crawford', \"'s\", 'film', ',', 'and', 'she', 'absolutely', 'shines', 'in', 'it', '.', 'Made', 'when', 'she', 'was', 'just', '27', ',', 'this', 'lesser', '-', 'known', 'version', 'of', 'Crawford', 'will', 'probably', 'be', 'unrecognizable', 'to', 'those', 'more', 'familiar', 'with', 'her', 'later', 'work', '.', 'However', ',', 'here', 'is', 'proof', 'that', 'long', 'before', 'she', 'took', 'home', 'an', 'Oscar', 'for', 'Mildred', 'Pierce', ',', 'Crawford', 'was', 'a', 'star', 'in', 'the', 'true', 'sense', 'of', 'the', 'word', ',', 'a', 'terrific', 'actress', 'with', 'the', 'charisma', 'to', 'carry', 'a', 'picture', 'all', 'by', 'herself.<br', '/><br', '/>Score', ':', 'EIGHT', 'out', 'of', 'TEN']\n"
     ]
    }
   ],
   "source": [
    "print(train_valid_data[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRMuAOum3rB5",
    "outputId": "3a198d4a-7dc9-4d08-efdd-4864b69b0e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "print(train_valid_data[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgZgQbyD3u9D"
   },
   "source": [
    "### テストセット以外の部分を訓練データと検証データに分ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h2FtnEKZ32hM"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = train_valid_data.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fzsi9ZC36eR",
    "outputId": "ef1762c3-5ba5-4e88-984d-25a101610e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsLNP7pGaNtp",
    "outputId": "7bf7d341-263b-4f96-ac01-efb955e62b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'one', 'of', 'the', 'greatest', 'child', '-', 'pet', 'movies', 'ever', 'created', '.', 'I', 'cry', 'every', 'time', 'I', 'see', 'Shadow', 'yelling', '\"', 'Wait', ',', 'wait', 'for', 'me', 'Peter', '!', '\"', 'as', 'the', 'family', 'car', 'is', 'pulling', 'away', '.', 'This', 'is', 'a', 'must', 'see', 'if', 'you', 'love', 'animals', '!', 'Best', 'Movie', 'Ever', '!', 'The', 'lines', 'in', 'the', 'movie', 'are', 'sometimes', 'stupid', '.', 'Like', 'when', 'Sassy', 'says', 'to', 'Chance', ';', '\"', 'Cat', \"'s\", 'Rule', 'and', 'dogs', 'drool', '!', '\"', 'Lines', 'like', 'this', 'I', 'could', 'do', 'without', ',', 'but', 'when', 'I', 'was', 'six', 'I', 'bet', 'I', 'loved', 'that', 'line', '.', 'The', 'storyline', 'may', 'seem', 'hooky', 'to', 'some', ',', 'but', 'I', 'like', 'it', '.', 'Shadow', 'as', 'the', 'older', 'dog', 'who', \"'s\", 'preparing', 'Chance', 'to', 'take', 'over', 'for', 'him', 'when', 'he', \"'s\", 'gone', 'is', 'really', 'moving', 'when', 'you', 'think', 'about', 'it', '.', 'It', 'reminded', 'me', 'of', 'my', 'childhood', 'dog', '.', 'I', 'think', 'everyone', 'can', 'find', 'a', 'piece', 'of', 'themselves', 'in', '\"', 'Homeward', 'Bound', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oXz2lvB37Vm"
   },
   "source": [
    "### データセットの語彙とラベルを作る\n",
    "* TEXTラベルのほうでは、最大語彙サイズを指定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DBQeD7yC37x4"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000 # この値は適当。\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iv6RSh3HmLf"
   },
   "source": [
    "なぜ語彙サイズが25,000ではなく25,002なのかについては、少し下の説明を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWuYQthC4Ml8",
    "outputId": "7a8bc91e-1364-4f87-9775-5c7349cfdb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW4eR-K44Rba"
   },
   "source": [
    "### 出現頻度順で上位２０単語を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jan98ffr4PXP",
    "outputId": "a61b35c1-c7b4-4375-a1ca-74ba8d865197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 231466), (',', 220572), ('.', 189097), ('a', 125017), ('and', 125012), ('of', 114875), ('to', 106992), ('is', 87340), ('in', 70294), ('I', 61900), ('it', 61164), ('that', 56231), ('\"', 50199), (\"'s\", 49490), ('this', 48385), ('-', 42420), ('/><br', 40924), ('was', 40198), ('as', 34764), ('with', 34223)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKQojOuv4Z38"
   },
   "source": [
    "### 単語ID順に最初の１０単語を見てみる\n",
    "* IDのうち、0と1は、未知語とパディング用の単語という特殊な単語に割り振られている。\n",
    " * 未知語は`<unk>`という特殊な単語に置き換えられる。これのIDが0。\n",
    " * パディングとは、長さが不揃いの複数の文書を同じミニバッチにまとめるとき、すべての文書の長さを無理やりそろえるため、文書末尾に特殊な単語（元々の語彙にない、人工的に用意した単語）を追加すること。\n",
    " * パディング用の単語が`<pad>`になっているのは、上のほうで使ったFieldクラスのインスタンスを作るときのデフォルトの値がこの`<pad>`になっているため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlhXRT3g4Xad",
    "outputId": "8dc01b28-dff5-421f-dfd9-08550ef23486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vJfHTdR4qd4"
   },
   "source": [
    "### ラベルのほうのIDを確認する\n",
    "* こちらはnegとposに対応する２つのIDしかない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vI7Pz_6R4bYM",
    "outputId": "c0c9acb8-7dc4-4e11-e501-10531c5492d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14_znTjp4w5s"
   },
   "source": [
    "### ミニバッチを取り出すためのiteratorを作る\n",
    "* ミニバッチのサイズを指定する。\n",
    " * ミニバッチのサイズは、性能を出すためにチューニングすべきハイパーパラメータのひとつ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cUED86Jb4tUy"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device = torch.device(\"cuda\", 2)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator = data.BucketIterator(train_data, batch_size=BATCH_SIZE, device=device,\n",
    "                                     sort_within_batch=True, shuffle=True, sort_key=lambda x: len(x.text))\n",
    "valid_iterator = data.BucketIterator(valid_data, batch_size=BATCH_SIZE, device=device)\n",
    "test_iterator = data.BucketIterator(test_data, batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a45QA7ncg_Qv"
   },
   "source": [
    "### ミニバッチの中身を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAW9Ec5q6BQO"
   },
   "source": [
    "* 訓練データのiteratorを回してミニバッチをすべて取得してみる\n",
    " * ミニバッチのshapeを表示してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kpn4tfWl42kY",
    "outputId": "2d8fec38-cd02-49d6-ae54-e0e50699f393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([133, 100])\n",
      "1 torch.Size([454, 100])\n",
      "2 torch.Size([248, 100])\n",
      "3 torch.Size([213, 100])\n",
      "4 torch.Size([106, 100])\n",
      "5 torch.Size([374, 100])\n",
      "6 torch.Size([94, 100])\n",
      "7 torch.Size([187, 100])\n",
      "8 torch.Size([286, 100])\n",
      "9 torch.Size([112, 100])\n",
      "10 torch.Size([49, 100])\n",
      "11 torch.Size([531, 100])\n",
      "12 torch.Size([190, 100])\n",
      "13 torch.Size([161, 100])\n",
      "14 torch.Size([227, 100])\n",
      "15 torch.Size([135, 100])\n",
      "16 torch.Size([386, 100])\n",
      "17 torch.Size([829, 100])\n",
      "18 torch.Size([182, 100])\n",
      "19 torch.Size([170, 100])\n",
      "20 torch.Size([243, 100])\n",
      "21 torch.Size([921, 100])\n",
      "22 torch.Size([1055, 100])\n",
      "23 torch.Size([69, 100])\n",
      "24 torch.Size([300, 100])\n",
      "25 torch.Size([270, 100])\n",
      "26 torch.Size([329, 100])\n",
      "27 torch.Size([216, 100])\n",
      "28 torch.Size([612, 100])\n",
      "29 torch.Size([128, 100])\n",
      "30 torch.Size([275, 100])\n",
      "31 torch.Size([223, 100])\n",
      "32 torch.Size([154, 100])\n",
      "33 torch.Size([472, 100])\n",
      "34 torch.Size([63, 100])\n",
      "35 torch.Size([175, 100])\n",
      "36 torch.Size([398, 100])\n",
      "37 torch.Size([438, 100])\n",
      "38 torch.Size([121, 100])\n",
      "39 torch.Size([695, 100])\n",
      "40 torch.Size([410, 100])\n",
      "41 torch.Size([201, 100])\n",
      "42 torch.Size([166, 100])\n",
      "43 torch.Size([157, 100])\n",
      "44 torch.Size([346, 100])\n",
      "45 torch.Size([581, 100])\n",
      "46 torch.Size([422, 100])\n",
      "47 torch.Size([265, 100])\n",
      "48 torch.Size([145, 100])\n",
      "49 torch.Size([56, 100])\n",
      "50 torch.Size([100, 100])\n",
      "51 torch.Size([142, 100])\n",
      "52 torch.Size([294, 100])\n",
      "53 torch.Size([149, 100])\n",
      "54 torch.Size([259, 100])\n",
      "55 torch.Size([159, 100])\n",
      "56 torch.Size([254, 100])\n",
      "57 torch.Size([365, 100])\n",
      "58 torch.Size([179, 100])\n",
      "59 torch.Size([118, 100])\n",
      "60 torch.Size([490, 100])\n",
      "61 torch.Size([164, 100])\n",
      "62 torch.Size([184, 100])\n",
      "63 torch.Size([206, 100])\n",
      "64 torch.Size([749, 100])\n",
      "65 torch.Size([210, 100])\n",
      "66 torch.Size([177, 100])\n",
      "67 torch.Size([152, 100])\n",
      "68 torch.Size([236, 100])\n",
      "69 torch.Size([281, 100])\n",
      "70 torch.Size([653, 100])\n",
      "71 torch.Size([141, 100])\n",
      "72 torch.Size([230, 100])\n",
      "73 torch.Size([156, 100])\n",
      "74 torch.Size([239, 100])\n",
      "75 torch.Size([203, 100])\n",
      "76 torch.Size([356, 100])\n",
      "77 torch.Size([337, 100])\n",
      "78 torch.Size([150, 100])\n",
      "79 torch.Size([313, 100])\n",
      "80 torch.Size([168, 100])\n",
      "81 torch.Size([75, 100])\n",
      "82 torch.Size([198, 100])\n",
      "83 torch.Size([81, 100])\n",
      "84 torch.Size([220, 100])\n",
      "85 torch.Size([88, 100])\n",
      "86 torch.Size([147, 100])\n",
      "87 torch.Size([131, 100])\n",
      "88 torch.Size([1996, 100])\n",
      "89 torch.Size([555, 100])\n",
      "90 torch.Size([144, 100])\n",
      "91 torch.Size([137, 100])\n",
      "92 torch.Size([139, 100])\n",
      "93 torch.Size([172, 100])\n",
      "94 torch.Size([195, 100])\n",
      "95 torch.Size([125, 100])\n",
      "96 torch.Size([322, 100])\n",
      "97 torch.Size([305, 100])\n",
      "98 torch.Size([508, 100])\n",
      "99 torch.Size([192, 100])\n",
      "100 torch.Size([240, 100])\n",
      "101 torch.Size([200, 100])\n",
      "102 torch.Size([224, 100])\n",
      "103 torch.Size([231, 100])\n",
      "104 torch.Size([105, 100])\n",
      "105 torch.Size([133, 100])\n",
      "106 torch.Size([153, 100])\n",
      "107 torch.Size([220, 100])\n",
      "108 torch.Size([562, 100])\n",
      "109 torch.Size([111, 100])\n",
      "110 torch.Size([189, 100])\n",
      "111 torch.Size([228, 100])\n",
      "112 torch.Size([390, 100])\n",
      "113 torch.Size([186, 100])\n",
      "114 torch.Size([269, 100])\n",
      "115 torch.Size([156, 100])\n",
      "116 torch.Size([324, 100])\n",
      "117 torch.Size([141, 100])\n",
      "118 torch.Size([155, 100])\n",
      "119 torch.Size([254, 100])\n",
      "120 torch.Size([1044, 100])\n",
      "121 torch.Size([194, 100])\n",
      "122 torch.Size([539, 100])\n",
      "123 torch.Size([663, 100])\n",
      "124 torch.Size([249, 100])\n",
      "125 torch.Size([169, 100])\n",
      "126 torch.Size([309, 100])\n",
      "127 torch.Size([1698, 100])\n",
      "128 torch.Size([591, 100])\n",
      "129 torch.Size([144, 100])\n",
      "130 torch.Size([341, 100])\n",
      "131 torch.Size([149, 100])\n",
      "132 torch.Size([417, 100])\n",
      "133 torch.Size([213, 100])\n",
      "134 torch.Size([478, 100])\n",
      "135 torch.Size([94, 100])\n",
      "136 torch.Size([165, 100])\n",
      "137 torch.Size([63, 100])\n",
      "138 torch.Size([183, 100])\n",
      "139 torch.Size([206, 100])\n",
      "140 torch.Size([825, 100])\n",
      "141 torch.Size([197, 100])\n",
      "142 torch.Size([301, 100])\n",
      "143 torch.Size([217, 100])\n",
      "144 torch.Size([68, 100])\n",
      "145 torch.Size([498, 100])\n",
      "146 torch.Size([443, 100])\n",
      "147 torch.Size([380, 100])\n",
      "148 torch.Size([371, 100])\n",
      "149 torch.Size([124, 100])\n",
      "150 torch.Size([236, 100])\n",
      "151 torch.Size([127, 100])\n",
      "152 torch.Size([264, 100])\n",
      "153 torch.Size([287, 100])\n",
      "154 torch.Size([294, 100])\n",
      "155 torch.Size([120, 100])\n",
      "156 torch.Size([75, 100])\n",
      "157 torch.Size([88, 100])\n",
      "158 torch.Size([350, 100])\n",
      "159 torch.Size([142, 100])\n",
      "160 torch.Size([167, 100])\n",
      "161 torch.Size([191, 100])\n",
      "162 torch.Size([172, 100])\n",
      "163 torch.Size([623, 100])\n",
      "164 torch.Size([430, 100])\n",
      "165 torch.Size([754, 100])\n",
      "166 torch.Size([147, 100])\n",
      "167 torch.Size([137, 100])\n",
      "168 torch.Size([81, 100])\n",
      "169 torch.Size([146, 100])\n",
      "170 torch.Size([100, 100])\n",
      "171 torch.Size([49, 100])\n",
      "172 torch.Size([117, 100])\n",
      "173 torch.Size([174, 100])\n",
      "174 torch.Size([203, 100])\n",
      "175 torch.Size([276, 100])\n",
      "176 torch.Size([135, 100])\n",
      "177 torch.Size([332, 100])\n",
      "178 torch.Size([163, 100])\n",
      "179 torch.Size([181, 100])\n",
      "180 torch.Size([403, 100])\n",
      "181 torch.Size([160, 100])\n",
      "182 torch.Size([161, 100])\n",
      "183 torch.Size([361, 100])\n",
      "184 torch.Size([176, 100])\n",
      "185 torch.Size([158, 100])\n",
      "186 torch.Size([130, 100])\n",
      "187 torch.Size([915, 100])\n",
      "188 torch.Size([316, 100])\n",
      "189 torch.Size([210, 100])\n",
      "190 torch.Size([178, 100])\n",
      "191 torch.Size([282, 100])\n",
      "192 torch.Size([244, 100])\n",
      "193 torch.Size([56, 100])\n",
      "194 torch.Size([151, 100])\n",
      "195 torch.Size([517, 100])\n",
      "196 torch.Size([702, 100])\n",
      "197 torch.Size([259, 100])\n",
      "198 torch.Size([461, 100])\n",
      "199 torch.Size([139, 100])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "  print(i, batch.text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWW1np1P6OQg"
   },
   "source": [
    "* ミニバッチの形は、[ミニバッチ内での最大文書長, ミニバッチのサイズ]になっていることに注意！\n",
    " * ミニバッチのサイズが最初に来ているのではない！\n",
    " * [ミニバッチのサイズ, ミニバッチ内での最大文書長]という形にしたいなら、テキストのfieldを作るとき以下のようにする。\n",
    "\n",
    "__`TEXT = data.Field(tokenize=\"spacy\", batch_first=True)`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHytOsiSUdeS"
   },
   "source": [
    "* 上記のループを抜けたあとには、変数batchには最後のミニバッチが代入されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d78vJW616H7m",
    "outputId": "914d591f-3402-4c3b-e564-45885b50b062"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([139, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHMHkR73VuCD"
   },
   "source": [
    "* このミニバッチに含まれる文書のうち、最初の文書の単語ID列を表示させてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tZLm0hQVjZE",
    "outputId": "7d47a7b0-b5c7-429c-b6a2-5a5e72f69a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   11,   216,    14,    25,  4676,     7, 17659,    14,    19,     5,\n",
      "           62,    59, 17411,   710,    22,     3,    21,    60,  3409,   375,\n",
      "           45,  1460,  8578,     3,  2819, 24356,     6,  3640,     0,     4,\n",
      "          173,    16,    78,    43,   426,     7,     2,   225,    29,   239,\n",
      "            2,  1328,    30,    12,    63,   264,    36,    38,     7,    83,\n",
      "         1668,   786,    17,   664,   569,     7,     2,  5589,    18, 13953,\n",
      "            3,    11,   160,   236,  6596, 24356,     8,  1525,     3,   450,\n",
      "         5012,     0,    85,    27,   952,    49,    20,    95,    23,    90,\n",
      "            7,     2,   225,     6,    72,    85,    34,     8,  4498,     2,\n",
      "         7731,   164,     0,   130,     4,   173,    31,   217,    54,     2,\n",
      "            0,  1328,    52,     2, 12691,     6,     2,    14, 15005,    14,\n",
      "          175,    10,   172,   158,     3,   123,    16,    22,     9,  1184,\n",
      "          590,     4,  3401,    83,   924,  1668,     7,     2,   774,   117,\n",
      "            3,    26,    10,    82,   830,    99,    41,    42,     4],\n",
      "       device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(batch.text[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcXV1WLCdax1"
   },
   "source": [
    "* このミニバッチに含まれる文書のうち、最初の文書の単語ID列を単語列に戻したものを表示させてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjaS-Mjadf63",
    "outputId": "fdf67d7b-df80-46cd-d540-4d51c59837a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought \" The River of Souls \" was a very good Babylon 5 movie , with some exceptional performances from Martin Sheen , Tracy Scoggins and Ian <unk> . If this were an episode of the series ( without the humour ) it would probably be one of my favourite stand - alone stories of the series.<br /><br />Personally , I 've always preferred Scoggins to Christian , although granted <unk> did n't write her as well for much of the series and she did have to endure the Byron / <unk> plot . If you take out the <unk> humour about the brothel and the \" poorer \" actors in those scenes , then this movie is solid stuff . Probably my third favourite of the four movies , but in no means bad at all .\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uoslpyTgz8w"
   },
   "source": [
    "* このミニバッチに含まれる文書のうち、最後の文書の単語ID列を表示させてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcdyIhK0TUac",
    "outputId": "42ca310a-3459-4015-a6d7-8d079e12e378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   11,    34,    91,     5,   356,     7,    16,    22,    23,   176,\n",
      "            6,   176,     4,  1448,     7, 16091,     0,   875,    93,     2,\n",
      "        20137,     3,    11,    74,     8,   217,     2,    22,   142,     2,\n",
      "         4367,     6,   126,    12,     4,   567,   101,   169,    10,  5920,\n",
      "           85,    33,    81,   112,   425,    16,    22,    19,    10,   818,\n",
      "           84,    58,  3733,  6274,     6,  1916,  6953,    32,   818,  3004,\n",
      "            4,   764,    11,   133,    34,   178,  9864,  2183, 11328,     3,\n",
      "           16,    22,  1146,    84,   209,    68,    11,   126,    12,     4,\n",
      "           25,   131,     9,   377,     8,   805,    23,     5,   248,     6,\n",
      "            2,   527,     9,    48,  1475,     4,    11,   648,   398,    13,\n",
      "           31,   126,    16,    76,    31,    34,    27,   494,     4,   240,\n",
      "           96,    94,     8,    81,  3165, 12228,     3,  5487,  3889,     3,\n",
      "        16091, 18484,     3,     6,  4427,  3886,    88,     8,   445,     5,\n",
      "          186,    88,    42,   227,  3851,   375,     4,     1,     1],\n",
      "       device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(batch.text[:, BATCH_SIZE-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtDXRKPMT9KW"
   },
   "source": [
    "最後の文書の末尾は「1」で埋められていることが分かる。\n",
    "\n",
    "この1は、パディング用単語のIDだったことを想起されたい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDzk2ghCUD8N"
   },
   "source": [
    "ミニバッチに含まれる文書の長さを調べると、文書が文書長の降順に並べられていることが分かる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PutP_EU4Tca-",
    "outputId": "09edc9dc-5d67-4782-94c5-0c210a2f5d78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139, 139,\n",
       "        139, 139, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138,\n",
       "        138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138,\n",
       "        138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138,\n",
       "        138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138,\n",
       "        138, 138, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137,\n",
       "        137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137, 137,\n",
       "        137, 137], device='cuda:2')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch.text != 1).sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PDZlF0O6doP"
   },
   "source": [
    "## 07-02 MLPによる文書分類の準備\n",
    "* 今回は、ごく簡単なMLPで文書分類をする。\n",
    "* 文書中の全単語トークンの埋め込みベクトルの平均を、MLPの入力とする。\n",
    " * 当然、語順の情報は使われない。\n",
    " * つまり、bag-of-wordsモデルになっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjpel2i46gbD"
   },
   "source": [
    "### 定数の設定\n",
    "* 単語埋め込みベクトルの次元数は128にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQPXVLC66NUM",
    "outputId": "dcf81671-9ef1-4c94-f603-a68b08602faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙サイズ 25002, クラス数 2, 単語埋め込み次元 128\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "NUM_CLASS = len(LABEL.vocab)\n",
    "EMBED_DIM = 128\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "print(f'語彙サイズ {INPUT_DIM}, クラス数 {NUM_CLASS}, 単語埋め込み次元 {EMBED_DIM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsuHjuNp6tvt"
   },
   "source": [
    "### モデルを定義する前にPyTorchの単語埋め込みがどんなものかを見てみる\n",
    "* 埋め込みとは、単語IDから単語ベクトルへのマッピング。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3J7TzxFVMsR"
   },
   "source": [
    "* 以下のように、語彙サイズと埋め込みの次元数を指定しつつ、torch.nn.Embeddingのインスタンスを作ればよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QP7jJVYT6tBg"
   },
   "outputs": [],
   "source": [
    "embed = nn.Embedding(INPUT_DIM, EMBED_DIM, padding_idx=PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUl6lR8JVWTu"
   },
   "source": [
    "* パディング用の単語の埋め込みはゼロベクトルになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3ZCr9Ll61m8",
    "outputId": "6549fd48-9898-434f-ed24-fa8a705a1725"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-040ece04b708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "print(embed(torch.tensor([21,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN0kM3urr-Il"
   },
   "source": [
    "* 埋め込みの効果を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQ0FqJRcr6Vo",
    "outputId": "2ca3fca1-8ea4-42b2-8999-c3e6e313bc99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([401, 100, 128])\n",
      "1 torch.Size([143, 100, 128])\n",
      "2 torch.Size([281, 100, 128])\n",
      "3 torch.Size([105, 100, 128])\n",
      "4 torch.Size([207, 100, 128])\n",
      "5 torch.Size([171, 100, 128])\n",
      "6 torch.Size([511, 100, 128])\n",
      "7 torch.Size([88, 100, 128])\n",
      "8 torch.Size([81, 100, 128])\n",
      "9 torch.Size([136, 100, 128])\n",
      "10 torch.Size([230, 100, 128])\n",
      "11 torch.Size([198, 100, 128])\n",
      "12 torch.Size([347, 100, 128])\n",
      "13 torch.Size([211, 100, 128])\n",
      "14 torch.Size([753, 100, 128])\n",
      "15 torch.Size([201, 100, 128])\n",
      "16 torch.Size([921, 100, 128])\n",
      "17 torch.Size([367, 100, 128])\n",
      "18 torch.Size([148, 100, 128])\n",
      "19 torch.Size([301, 100, 128])\n",
      "20 torch.Size([584, 100, 128])\n",
      "21 torch.Size([294, 100, 128])\n",
      "22 torch.Size([195, 100, 128])\n",
      "23 torch.Size([438, 100, 128])\n",
      "24 torch.Size([616, 100, 128])\n",
      "25 torch.Size([222, 100, 128])\n",
      "26 torch.Size([62, 100, 128])\n",
      "27 torch.Size([239, 100, 128])\n",
      "28 torch.Size([829, 100, 128])\n",
      "29 torch.Size([264, 100, 128])\n",
      "30 torch.Size([219, 100, 128])\n",
      "31 torch.Size([182, 100, 128])\n",
      "32 torch.Size([193, 100, 128])\n",
      "33 torch.Size([1041, 100, 128])\n",
      "34 torch.Size([188, 100, 128])\n",
      "35 torch.Size([76, 100, 128])\n",
      "36 torch.Size([123, 100, 128])\n",
      "37 torch.Size([276, 100, 128])\n",
      "38 torch.Size([163, 100, 128])\n",
      "39 torch.Size([157, 100, 128])\n",
      "40 torch.Size([173, 100, 128])\n",
      "41 torch.Size([329, 100, 128])\n",
      "42 torch.Size([146, 100, 128])\n",
      "43 torch.Size([474, 100, 128])\n",
      "44 torch.Size([112, 100, 128])\n",
      "45 torch.Size([322, 100, 128])\n",
      "46 torch.Size([169, 100, 128])\n",
      "47 torch.Size([117, 100, 128])\n",
      "48 torch.Size([176, 100, 128])\n",
      "49 torch.Size([314, 100, 128])\n",
      "50 torch.Size([132, 100, 128])\n",
      "51 torch.Size([388, 100, 128])\n",
      "52 torch.Size([134, 100, 128])\n",
      "53 torch.Size([287, 100, 128])\n",
      "54 torch.Size([159, 100, 128])\n",
      "55 torch.Size([141, 100, 128])\n",
      "56 torch.Size([214, 100, 128])\n",
      "57 torch.Size([657, 100, 128])\n",
      "58 torch.Size([270, 100, 128])\n",
      "59 torch.Size([375, 100, 128])\n",
      "60 torch.Size([145, 100, 128])\n",
      "61 torch.Size([180, 100, 128])\n",
      "62 torch.Size([226, 100, 128])\n",
      "63 torch.Size([156, 100, 128])\n",
      "64 torch.Size([165, 100, 128])\n",
      "65 torch.Size([154, 100, 128])\n",
      "66 torch.Size([150, 100, 128])\n",
      "67 torch.Size([338, 100, 128])\n",
      "68 torch.Size([234, 100, 128])\n",
      "69 torch.Size([185, 100, 128])\n",
      "70 torch.Size([533, 100, 128])\n",
      "71 torch.Size([161, 100, 128])\n",
      "72 torch.Size([248, 100, 128])\n",
      "73 torch.Size([702, 100, 128])\n",
      "74 torch.Size([204, 100, 128])\n",
      "75 torch.Size([308, 100, 128])\n",
      "76 torch.Size([138, 100, 128])\n",
      "77 torch.Size([457, 100, 128])\n",
      "78 torch.Size([254, 100, 128])\n",
      "79 torch.Size([167, 100, 128])\n",
      "80 torch.Size([130, 100, 128])\n",
      "81 torch.Size([49, 100, 128])\n",
      "82 torch.Size([99, 100, 128])\n",
      "83 torch.Size([94, 100, 128])\n",
      "84 torch.Size([69, 100, 128])\n",
      "85 torch.Size([56, 100, 128])\n",
      "86 torch.Size([259, 100, 128])\n",
      "87 torch.Size([560, 100, 128])\n",
      "88 torch.Size([243, 100, 128])\n",
      "89 torch.Size([120, 100, 128])\n",
      "90 torch.Size([1743, 100, 128])\n",
      "91 torch.Size([190, 100, 128])\n",
      "92 torch.Size([140, 100, 128])\n",
      "93 torch.Size([413, 100, 128])\n",
      "94 torch.Size([127, 100, 128])\n",
      "95 torch.Size([177, 100, 128])\n",
      "96 torch.Size([426, 100, 128])\n",
      "97 torch.Size([493, 100, 128])\n",
      "98 torch.Size([152, 100, 128])\n",
      "99 torch.Size([358, 100, 128])\n",
      "100 torch.Size([157, 100, 128])\n",
      "101 torch.Size([823, 100, 128])\n",
      "102 torch.Size([358, 100, 128])\n",
      "103 torch.Size([427, 100, 128])\n",
      "104 torch.Size([259, 100, 128])\n",
      "105 torch.Size([197, 100, 128])\n",
      "106 torch.Size([191, 100, 128])\n",
      "107 torch.Size([171, 100, 128])\n",
      "108 torch.Size([141, 100, 128])\n",
      "109 torch.Size([173, 100, 128])\n",
      "110 torch.Size([212, 100, 128])\n",
      "111 torch.Size([125, 100, 128])\n",
      "112 torch.Size([914, 100, 128])\n",
      "113 torch.Size([225, 100, 128])\n",
      "114 torch.Size([221, 100, 128])\n",
      "115 torch.Size([657, 100, 128])\n",
      "116 torch.Size([316, 100, 128])\n",
      "117 torch.Size([56, 100, 128])\n",
      "118 torch.Size([237, 100, 128])\n",
      "119 torch.Size([218, 100, 128])\n",
      "120 torch.Size([537, 100, 128])\n",
      "121 torch.Size([134, 100, 128])\n",
      "122 torch.Size([618, 100, 128])\n",
      "123 torch.Size([294, 100, 128])\n",
      "124 torch.Size([324, 100, 128])\n",
      "125 torch.Size([143, 100, 128])\n",
      "126 torch.Size([180, 100, 128])\n",
      "127 torch.Size([183, 100, 128])\n",
      "128 torch.Size([175, 100, 128])\n",
      "129 torch.Size([493, 100, 128])\n",
      "130 torch.Size([166, 100, 128])\n",
      "131 torch.Size([129, 100, 128])\n",
      "132 torch.Size([275, 100, 128])\n",
      "133 torch.Size([153, 100, 128])\n",
      "134 torch.Size([112, 100, 128])\n",
      "135 torch.Size([214, 100, 128])\n",
      "136 torch.Size([228, 100, 128])\n",
      "137 torch.Size([140, 100, 128])\n",
      "138 torch.Size([208, 100, 128])\n",
      "139 torch.Size([413, 100, 128])\n",
      "140 torch.Size([379, 100, 128])\n",
      "141 torch.Size([118, 100, 128])\n",
      "142 torch.Size([458, 100, 128])\n",
      "143 torch.Size([401, 100, 128])\n",
      "144 torch.Size([332, 100, 128])\n",
      "145 torch.Size([178, 100, 128])\n",
      "146 torch.Size([240, 100, 128])\n",
      "147 torch.Size([269, 100, 128])\n",
      "148 torch.Size([281, 100, 128])\n",
      "149 torch.Size([88, 100, 128])\n",
      "150 torch.Size([74, 100, 128])\n",
      "151 torch.Size([475, 100, 128])\n",
      "152 torch.Size([68, 100, 128])\n",
      "153 torch.Size([150, 100, 128])\n",
      "154 torch.Size([168, 100, 128])\n",
      "155 torch.Size([442, 100, 128])\n",
      "156 torch.Size([105, 100, 128])\n",
      "157 torch.Size([193, 100, 128])\n",
      "158 torch.Size([148, 100, 128])\n",
      "159 torch.Size([122, 100, 128])\n",
      "160 torch.Size([233, 100, 128])\n",
      "161 torch.Size([205, 100, 128])\n",
      "162 torch.Size([254, 100, 128])\n",
      "163 torch.Size([200, 100, 128])\n",
      "164 torch.Size([49, 100, 128])\n",
      "165 torch.Size([136, 100, 128])\n",
      "166 torch.Size([138, 100, 128])\n",
      "167 torch.Size([244, 100, 128])\n",
      "168 torch.Size([164, 100, 128])\n",
      "169 torch.Size([162, 100, 128])\n",
      "170 torch.Size([752, 100, 128])\n",
      "171 torch.Size([249, 100, 128])\n",
      "172 torch.Size([160, 100, 128])\n",
      "173 torch.Size([158, 100, 128])\n",
      "174 torch.Size([132, 100, 128])\n",
      "175 torch.Size([94, 100, 128])\n",
      "176 torch.Size([155, 100, 128])\n",
      "177 torch.Size([151, 100, 128])\n",
      "178 torch.Size([369, 100, 128])\n",
      "179 torch.Size([188, 100, 128])\n",
      "180 torch.Size([585, 100, 128])\n",
      "181 torch.Size([557, 100, 128])\n",
      "182 torch.Size([100, 100, 128])\n",
      "183 torch.Size([1067, 100, 128])\n",
      "184 torch.Size([287, 100, 128])\n",
      "185 torch.Size([146, 100, 128])\n",
      "186 torch.Size([63, 100, 128])\n",
      "187 torch.Size([389, 100, 128])\n",
      "188 torch.Size([300, 100, 128])\n",
      "189 torch.Size([695, 100, 128])\n",
      "190 torch.Size([307, 100, 128])\n",
      "191 torch.Size([203, 100, 128])\n",
      "192 torch.Size([349, 100, 128])\n",
      "193 torch.Size([341, 100, 128])\n",
      "194 torch.Size([1996, 100, 128])\n",
      "195 torch.Size([186, 100, 128])\n",
      "196 torch.Size([515, 100, 128])\n",
      "197 torch.Size([145, 100, 128])\n",
      "198 torch.Size([264, 100, 128])\n",
      "199 torch.Size([81, 100, 128])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "  print(i, embed(batch.text).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyngitc78hv"
   },
   "source": [
    "### モデルの定義\n",
    "* MLP（多層パーセプトロン）だが、入り口に単語埋め込み層が挿入されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9asdLYng7DOu"
   },
   "outputs": [],
   "source": [
    "class EmbedTextSentiment(nn.Module):\n",
    "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx):\n",
    "    super(EmbedTextSentiment, self).__init__()\n",
    "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "    self.fc1 = nn.Linear(embed_dim, 500)\n",
    "    self.fc2 = nn.Linear(500, 100)\n",
    "    self.fc3 = nn.Linear(100, num_class)\n",
    "\n",
    "  def forward(self, text):\n",
    "    x = self.embed(text)\n",
    "    x = x.mean(0) # 文書に含まれる全単語トークンの単語ベクトルの平均\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foU72cB48IO9"
   },
   "source": [
    "### モデルを作る\n",
    "* モデル（のインスタンス）をGPUに移動させている点に注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "W0BHCGAZ8F18"
   },
   "outputs": [],
   "source": [
    "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wylQOq8N8cqI"
   },
   "source": [
    "### 損失関数とoptimizerとschedulerを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "gw34INS78cIW"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilWLfu8Z8MzW"
   },
   "source": [
    "### 訓練用の関数\n",
    "* 最初の`model.train()`に注意。こうやって、モデルを訓練モードに設定する。\n",
    " * 例えば、dropoutを含むモデルなど、訓練時と評価時で、ふるまい方を変える必要があるときがあるため、こういうことをする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "UR2R4Lqh8J7n"
   },
   "outputs": [],
   "source": [
    "def train(data_iterator, model, optimizer, scheduler, criterion):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  train_loss = 0\n",
    "  train_acc = 0\n",
    "  for batch in data_iterator:\n",
    "    optimizer.zero_grad()\n",
    "    text, cls = batch.text, batch.label\n",
    "    output = model(text)\n",
    "    loss = criterion(output, cls)\n",
    "    train_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_acc += (output.argmax(1) == cls).float().mean().item()\n",
    "\n",
    "  scheduler.step()\n",
    "\n",
    "  num_batch = len(data_iterator)\n",
    "  return train_loss / num_batch, train_acc / num_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftuX8e1W8iRh"
   },
   "source": [
    "### 評価用の関数\n",
    "* 最初の`model.eval()`に注意。こうやって、モデルを評価モードに設定する。\n",
    " * 例えば、dropoutを含むモデルなど、訓練時と評価時で、ふるまい方を変える必要があるときがあるため、こういうことをする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "wGUnsJlq8Ue3"
   },
   "outputs": [],
   "source": [
    "def test(data_iterator, model, criterion):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  loss = 0\n",
    "  acc = 0\n",
    "  for batch in data_iterator:\n",
    "    text, cls = batch.text, batch.label\n",
    "    with torch.no_grad():\n",
    "      output = model(text)\n",
    "      loss = criterion(output, cls)\n",
    "      loss += loss.item()\n",
    "      acc += (output.argmax(1) == cls).float().mean().item()\n",
    "\n",
    "  num_batch = len(data_iterator)\n",
    "  return loss / num_batch, acc / num_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8o_jDAg8osP"
   },
   "source": [
    "## 07-03 分類器の訓練と検証セットでの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJJFv4k-8mH1",
    "outputId": "3d5d5659-968f-4a2c-dea2-a0fa8da1cafa"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Deterministic behavior was enabled with either `torch.set_deterministic(True)` or `at::Context::setDeterministic(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f26b77d33761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-b6d8259525d3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_iterator, model, optimizer, scheduler, criterion)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-57da353c5351>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 文書に含まれる全単語トークンの単語ベクトルの平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Deterministic behavior was enabled with either `torch.set_deterministic(True)` or `at::Context::setDeterministic(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "  start_time = time.time()\n",
    "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
    "  valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
    "\n",
    "  secs = int(time.time() - start_time)\n",
    "  mins = secs // 60\n",
    "  secs = secs % 60\n",
    "\n",
    "  print(f'Epoch {epoch + 1} | time in {mins:d} minutes, {secs:d} seconds | ', end='')\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(f'lr={param_group[\"lr\"]:.6f}')\n",
    "    break\n",
    "  print(f'\\tLoss: {train_loss:.5f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
    "  print(f'\\tLoss: {valid_loss:.5f}(valid)\\t|\\tAcc: {valid_acc * 100:.2f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPux8PReWTXG"
   },
   "source": [
    "## 07-04 再検討\n",
    "* 訓練データ上での分類精度がほぼ100%になってしまっている。\n",
    "* 検証データでの分類精度と大きな差があり、明らかにオーバーフィッティング。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23jMgtmoWkty"
   },
   "source": [
    "### ドロップアウトを使う\n",
    "* モデルのインスタンスを作るときにdropoutの確率を引数pで指定できるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khps3ZuBWntq"
   },
   "outputs": [],
   "source": [
    "class EmbedTextSentiment(nn.Module):\n",
    "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx, p=0.0):\n",
    "    super(EmbedTextSentiment, self).__init__()\n",
    "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "    self.dropout = nn.Dropout(p=p)\n",
    "    self.fc1 = nn.Linear(embed_dim, 500)\n",
    "    self.fc2 = nn.Linear(500, 100)\n",
    "    self.fc3 = nn.Linear(100, num_class)\n",
    "\n",
    "  def forward(self, text):\n",
    "    x = self.dropout(self.embed(text)) #埋め込み層の直後にdropout\n",
    "    x = x.mean(0)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVXbkt6qXxNt"
   },
   "outputs": [],
   "source": [
    "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXkBDXc6X1mp"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "  start_time = time.time()\n",
    "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
    "  valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
    "\n",
    "  secs = int(time.time() - start_time)\n",
    "  mins = secs // 60\n",
    "  secs = secs % 60\n",
    "\n",
    "  print(f'Epoch {epoch + 1} | time in {mins:d} minutes, {secs:d} seconds | ', end='')\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(f'lr={param_group[\"lr\"]:.6f}')\n",
    "    break\n",
    "  print(f'\\tLoss: {train_loss:.5f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
    "  print(f'\\tLoss: {valid_loss:.5f}(valid)\\t|\\tAcc: {valid_acc * 100:.2f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vu3Y-wjwb0po"
   },
   "source": [
    "### L２正則化を使う\n",
    "* optimizerのweight_decayパラメータを0より大きな値にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmxEuSFJazCJ"
   },
   "outputs": [],
   "source": [
    "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0Zr2S7ga3J4"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "  start_time = time.time()\n",
    "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
    "  valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
    "\n",
    "  secs = int(time.time() - start_time)\n",
    "  mins = secs // 60\n",
    "  secs = secs % 60\n",
    "\n",
    "  print(f'Epoch {epoch + 1} | time in {mins:d} minutes, {secs:d} seconds | ', end='')\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(f'lr={param_group[\"lr\"]:.6f}')\n",
    "    break\n",
    "  print(f'\\tLoss: {train_loss:.5f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
    "  print(f'\\tLoss: {valid_loss:.5f}(valid)\\t|\\tAcc: {valid_acc * 100:.2f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIHA64UTdmBj"
   },
   "source": [
    "### early stopping\n",
    "* dev setでのaccuracyが4回連続で最高値を下回ったら訓練を終えることにする。\n",
    "* early stoppingの実現については、PyTorch Lightningを使う手もある。\n",
    " * https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0zclQnVdlVZ"
   },
   "outputs": [],
   "source": [
    "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3E_I5sRc3FF"
   },
   "outputs": [],
   "source": [
    "patience = 4\n",
    "early_stop_count = 0\n",
    "best_valid_acc = 0.0\n",
    "\n",
    "MIN_N_EPOCHS = 10 # 最低このエポック数は実行する\n",
    "N_EPOCHS = 50 # エポック数を増やしておく\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "  start_time = time.time()\n",
    "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
    "  valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
    "\n",
    "  secs = int(time.time() - start_time)\n",
    "  mins = secs // 60\n",
    "  secs = secs % 60\n",
    "\n",
    "  print(f'Epoch {epoch + 1} | time in {mins:d} minutes, {secs:d} seconds | ', end='')\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(f'lr={param_group[\"lr\"]:.6f}')\n",
    "    break\n",
    "  print(f'\\tLoss: {train_loss:.5f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
    "  print(f'\\tLoss: {valid_loss:.5f}(valid)\\t|\\tAcc: {valid_acc * 100:.2f}%(valid)')\n",
    "\n",
    "  # early stopping\n",
    "  if epoch + 1 > MIN_N_EPOCHS:\n",
    "    if best_valid_acc <= valid_acc:\n",
    "      best_valid_acc = valid_acc\n",
    "      early_stop_count = 0\n",
    "    else:\n",
    "      early_stop_count += 1\n",
    "      if early_stop_count == patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRvkncN09MKk"
   },
   "source": [
    "## 07-05 テストセット上で評価\n",
    "* 見つけ出したベストな設定を使って、テストセット上での最終的な評価をおこなう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_gHj4x38y8h"
   },
   "outputs": [],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_iterator, model, criterion)\n",
    "print(f'\\tLoss: {test_loss:.5f}(test)\\t|\\tAcc: {test_acc * 100:.2f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1M_VQ1xhcWq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPT2CAq+Xa4AIVXeEz1X2nN",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "07_document_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
